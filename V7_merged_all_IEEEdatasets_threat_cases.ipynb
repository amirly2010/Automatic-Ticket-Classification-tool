{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8112936,
          "sourceType": "datasetVersion",
          "datasetId": 4792717
        },
        {
          "sourceId": 8114118,
          "sourceType": "datasetVersion",
          "datasetId": 4793617
        },
        {
          "sourceId": 33551,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 28083
        }
      ],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "V7_merged_all_IEEEdatasets_threat_cases",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirly2010/Automatic-Ticket-Classification-tool/blob/main/V7_merged_all_IEEEdatasets_threat_cases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'ieee-annotated:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4792717%2F8112936%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240605%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240605T011225Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5f879dca5b6fb9b59c0d09cd9a9ee834d2d1d1c0ed6aeae12b322757650ba341f8116eed0dbb68823efbf318fabfd1414c9c52491fa906f5f7e6c86ba2fb9fa772e6629b203c5b97b6d86ca4aebf38dd38a127e30b0abe1ed68aa650e29c93438a9ea831caa034982d4ffa1fbbd03fb056b1b32c13ae6ad745428cdd6fae7518cbc256e0f518b340969b30edd6af1501e319b746c603be300f8862a58bcaaf73c9d60c9d38deb0d4cdf2d4da36115822d0ae77394422a5838abbc4f5dbad40c336fc884fdf2695cf365e510a7c764aafd8ae5fb906ef7966c562ca7c9ae7a639af3c53e8e8a573dcaf189693f4b85cb69356bb0f173153c6a8fcb77ff1210abf,oensive-tweet:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4793617%2F8114118%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240605%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240605T011225Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9d96e46f8bacaa8566c558bf8cfdb700f52968ac27eeedbb893061ffb7708667fc724046f434311aa62643bad6ec55baff054fa66921714c12741b61bd916413bbb3a4e3cea2cb0c7b0500163288c8bb1cb117aa83a9d2fe8b899625b5d1850a8c6ce89c86105bb4f6f02e11bc33dfce0ec3cd0c6e301ac0d22d1a8bfbf36ab278eada0a3355301d58f489d9143974f060f48b8928b062c05ac31b6fda553267623960ff8fde34a6b736a24b295ea032acac207a618e0c850c6c419b5d0e6767acc9ca1c41cf4bde3f3bb9dd1822514a6de784332bdf1bc60a22d6b360b4aae81326633836259bb9c436de2fce8e8844c91e30a2177c7d49f6fa365f97ca6ea7,llama-3/transformers/8b-chat-hf/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F28083%2F33551%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240605%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240605T011225Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2395bdc69e90b6113bb242fd8543266756e18ab8558238cb7e8a35681653b14a0ced6724ed7626de3846d3e0714c43c0c7e68356aa8e75766e92304c6833828144b085cba5b2ac10b39e92b7d29cd32cf634cc9c9ba8fd97ef21d52bec1cebaa6b0bc9a7873ad2ad5858929aab5a9f859805e9a7a8d17f20d708515f8aad4ff312ccaeeff421b40557e0f00baea34dbeae18ddbee0cde4f4cd3c26317284010a5c51a0ca37a1b71dda0c9a0b27f3bb328c6dcab6091f1f8d055dd4b7e4230c9dc0d2a7b4fd327645a0f9457dbd322646fba639a7a5d02763fb48e9cf7e499569349443b178e81a5d3b8e922bd7bb430785711746ea9904066ebb8515af5a6a3b'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "y0jlevIY9G_o"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune Llama 2 for Sentiment Analysis\n",
        "\n",
        "For this hands-on tutorial on fine-tuning a Llama 2 model, I am going to deal with a sentiment analysis on financial and economic information. Sentiment analysis on financial and economic information is highly relevant for businesses for several key reasons, ranging from market insights (gain valuable insights into market trends, investor confidence, and consumer behavior) to risk management (identifying potential reputational risks) to investment decisions (gauging the sentiment of stakeholders, investors, and the general public businesses can assess the potential success of various investment opportunities).\n",
        "\n",
        "Before the technicalities of fine-tuning a large language model like Llama 2, we have to find the correct dataset to demonstrate the potentialities of fine-tuning.\n",
        "\n",
        "Particularly within the realm of finance and economic texts, annotated datasets are notably rare, with many being exclusively reserved for proprietary purposes. To address the issue of insufficient training data, scholars from the Aalto University School\n",
        "of Business introduced in 2014 a set of approximately 5000 sentences. This collection aimed to establish human-annotated benchmarks, serving as a standard for evaluating alternative modeling techniques. The involved annotators (16 people with\n",
        "adequate background knowledge on financial markets) were instructed to assess the sentences solely from the perspective of an investor, evaluating whether the news potentially holds a positive, negative, or neutral impact on the stock price.\n",
        "\n",
        "The FinancialPhraseBank dataset is a comprehensive collection that captures the sentiments of financial news headlines from the viewpoint of a retail investor. Comprising two key columns, namely \"Sentiment\" and \"News Headline,\" the dataset effectively classifies sentiments as either negative, neutral, or positive. This structured dataset serves as a valuable resource for analyzing and understanding the complex dynamics of sentiment in the domain of financial news. It has been used in various studies and research initiatives, since its inception in the work by Malo, P., Sinha, A., Korhonen, P., Wallenius, J., and Takala, P.  \"Good debt or bad debt: Detecting semantic orientations in economic texts.\", published in the Journal of the Association for Information Science and Technology in 2014."
      ],
      "metadata": {
        "id": "iwVMfQnd9G_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a first step, we install the specific libraries necessary to make this example work."
      ],
      "metadata": {
        "id": "5j3JVcLQ9G_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* accelerate is a distributed training library for PyTorch by HuggingFace. It allows you to train your models on multiple GPUs or CPUs in parallel (distributed configurations), which can significantly speed up training in presence of multiple GPUs (we won't use it in our example).\n",
        "* peft is a Python library by HuggingFace for efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT methods only fine-tune a small number of (extra) model parameters, thereby greatly decreasing the computational and storage costs.\n",
        "* bitsandbytes by Tim Dettmers, is a lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions. It allows to run models stored in 4-bit precision: while 4-bit bitsandbytes stores weights in 4-bits, the computation still happens in 16 or 32-bit and here any combination can be chosen (float16, bfloat16, float32, and so on).\n",
        "* transformers is a Python library for natural language processing (NLP). It provides a number of pre-trained models for NLP tasks such as text classification, question answering, and machine translation.\n",
        "* trl is a full stack library by HuggingFace providing a set of tools to train transformer language models with Reinforcement Learning, from the Supervised Fine-tuning step (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step."
      ],
      "metadata": {
        "id": "w5Bim3wz9G_1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "1pJgg3zi9G_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"IEEE_annotated---------------------------------\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(\"/kaggle/input/ieee-annotated/IEEE_annotated.csv\")\n",
        "\n",
        "# Filter rows where the \"annotation\" column is 'threat' or 'irrelevant'\n",
        "filtered_data = df[df['annotation'].isin(['threat', 'irrelevant', \"unknown\"])]\n",
        "\n",
        "# Select only the lowercase values from the \"type\" column\n",
        "filtered_data = filtered_data[filtered_data['type'].str.islower()]\n",
        "\n",
        "# Define a function to format labels based on annotation\n",
        "def format_label(annotation, label):\n",
        "    if annotation == 'threat':\n",
        "        return 'threat'\n",
        "    elif annotation == 'irrelevant':\n",
        "        return 'irrelevant'\n",
        "    else:\n",
        "        return 'unknown'\n",
        "\n",
        "# Apply the format_label function to create the \"label\" column\n",
        "filtered_data['label'] = filtered_data.apply(lambda row: format_label(row['annotation'], row['type']), axis=1)\n",
        "\n",
        "# Select only the specific labels\n",
        "selected_labels = [\n",
        "    'irrelevant',\n",
        "    'threat',\n",
        "    'unknown'\n",
        "]\n",
        "final_data = filtered_data[filtered_data['label'].isin(selected_labels)]\n",
        "\n",
        "# Select only the \"text\" and \"label\" columns\n",
        "final_data = final_data[['text', 'label']]\n",
        "\n",
        "# Count occurrences of each label\n",
        "label_counts = final_data['label'].value_counts()\n",
        "\n",
        "# Display label counts\n",
        "print(label_counts)\n",
        "\n",
        "# Save final data to CSV\n",
        "final_data.to_csv(\"/kaggle/working/final_data.csv\", index=False)\n",
        "\n",
        "# Display sample of the final data\n",
        "print(final_data.head())\n",
        "\n",
        "print(\"Oﬀensive_tweets---------------------------------\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a DataFrame\n",
        "df2 = pd.read_csv(\"/kaggle/input/oensive-tweet/Oensive_tweets.csv\")\n",
        "\n",
        "# Count occurrences of each label\n",
        "df2_counts = df2['is_off'].value_counts()\n",
        "\n",
        "# Display label counts\n",
        "print(df2_counts)\n",
        "\n",
        "# Filter rows where \"is_off\" is 1 and select only 2000 rows\n",
        "offensive_data = df2[df2['is_off'] == 1].head(500)\n",
        "\n",
        "# Drop the 'id' column\n",
        "offensive_data = offensive_data.drop(columns=['user_id'])\n",
        "\n",
        "# Rename the columns\n",
        "offensive_data = offensive_data.rename(columns={\"is_off\": \"label\", \"content\": \"text\"})\n",
        "\n",
        "# Rename the value 1 in the \"label\" column to \"threat: 'Offensive_Language'\"\n",
        "offensive_data['label'] = \"threat\"\n",
        "\n",
        "# Count occurrences of each label\n",
        "offensive_data_counts = offensive_data['label'].value_counts()\n",
        "\n",
        "# Display label counts\n",
        "print(offensive_data_counts)\n",
        "\n",
        "print(offensive_data.head())\n",
        "\n",
        "# Save the final DataFrame to a CSV file\n",
        "offensive_data.to_csv(\"final_offensive_data.csv\", index=False)\n",
        "\n",
        "print(\"merge IEEE_annotated-Oﬀensive_tweets---------------------------------\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the first dataset (final_data)\n",
        "final_data_df = pd.read_csv(\"/kaggle/working/final_data.csv\")\n",
        "\n",
        "# Load the second dataset (offensive_data)\n",
        "offensive_data_df = pd.read_csv(\"/kaggle/working/final_offensive_data.csv\")\n",
        "\n",
        "# Rename columns in offensive_data\n",
        "offensive_data_df = offensive_data_df[offensive_data_df['label'] == \"threat\"].head(500)\n",
        "offensive_data_df = offensive_data_df.rename(columns={\"label\": \"label\", \"text\": \"text\"})\n",
        "offensive_data_df['label'] = \"threat\"\n",
        "\n",
        "# Concatenate the two dataframes vertically\n",
        "merged_data = pd.concat([final_data_df, offensive_data_df], ignore_index=True)\n",
        "\n",
        "# Save the final DataFrame to a CSV file\n",
        "merged_data.to_csv(\"merge_IEEE_annotated_Oﬀensive_tweets.csv\", index=False)\n",
        "\n",
        "# Count occurrences of each label\n",
        "label_counts = merged_data['label'].value_counts()\n",
        "\n",
        "# Display label counts\n",
        "print(label_counts)\n",
        "\n",
        "\n",
        "\n",
        "# print(\"IEEE_annotated---------------------------------\")\n",
        "\n",
        "# import pandas as pd\n",
        "\n",
        "# # Load the CSV file into a DataFrame\n",
        "# df = pd.read_csv(\"/kaggle/input/ieee-annotated/IEEE_annotated.csv\")\n",
        "\n",
        "# # Filter rows where the \"annotation\" column is 'threat' or 'irrelevant'\n",
        "# filtered_data = df[df['annotation'].isin(['threat', 'irrelevant', \"unknown\"])]\n",
        "\n",
        "# # Select only the lowercase values from the \"type\" column\n",
        "# filtered_data = filtered_data[filtered_data['type'].str.islower()]\n",
        "\n",
        "# # Define a function to format labels based on annotation\n",
        "# def format_label(annotation, label):\n",
        "#     if annotation == 'threat':\n",
        "#         return f\"threat: {label.strip('[]').lower()}\"\n",
        "#     elif annotation == 'irrelevant':\n",
        "#         return 'irrelevant'\n",
        "#     else:\n",
        "#         return 'unknown'\n",
        "\n",
        "# # Apply the format_label function to create the \"label\" column\n",
        "# filtered_data['label'] = filtered_data.apply(lambda row: format_label(row['annotation'], row['type']), axis=1)\n",
        "\n",
        "# # Select only the specific labels\n",
        "# selected_labels = [\n",
        "#     'irrelevant',\n",
        "#     \"threat: 'vulnerability'\",\n",
        "#     \"threat: 'general'\",\n",
        "#     \"threat: 'ransomware'\",\n",
        "#     \"threat: 'ddos'\",\n",
        "#     \"threat: 'botnet'\",\n",
        "#     'unknown'\n",
        "# ]\n",
        "# final_data = filtered_data[filtered_data['label'].isin(selected_labels)]\n",
        "\n",
        "# # Select only the \"text\" and \"label\" columns\n",
        "# final_data = final_data[['text', 'label']]\n",
        "\n",
        "# # Count occurrences of each label\n",
        "# label_counts = final_data['label'].value_counts()\n",
        "\n",
        "# # Display label counts\n",
        "# print(label_counts)\n",
        "\n",
        "# # Save final data to CSV\n",
        "# final_data.to_csv(\"/kaggle/working/final_data.csv\", index=False)\n",
        "\n",
        "# # Display sample of the final data\n",
        "# print(final_data.head())\n",
        "\n",
        "\n",
        "\n",
        "# print(\"Oﬀensive_tweets---------------------------------\")\n",
        "\n",
        "\n",
        "# import pandas as pd\n",
        "\n",
        "# # Load the dataset into a DataFrame\n",
        "# df2 = pd.read_csv(\"/kaggle/input/oensive-tweet/Oensive_tweets.csv\")\n",
        "\n",
        "# # Count occurrences of each label\n",
        "# df2_counts = df2['is_off'].value_counts()\n",
        "\n",
        "# # Display label counts\n",
        "# print(df2_counts)\n",
        "\n",
        "# # Filter rows where \"is_off\" is 1 and select only 2000 rows\n",
        "# offensive_data = df2[df2['is_off'] == 1].head(500)\n",
        "\n",
        "# # Drop the 'id' column\n",
        "# offensive_data = offensive_data.drop(columns=['user_id'])\n",
        "\n",
        "# # Rename the columns\n",
        "# offensive_data = offensive_data.rename(columns={\"is_off\": \"label\", \"content\": \"text\"})\n",
        "\n",
        "# # Rename the value 1 in the \"label\" column to \"threat: 'Offensive_Language'\"\n",
        "# offensive_data['label'] = \"threat: 'Offensive_Language'\"\n",
        "\n",
        "# # Count occurrences of each label\n",
        "# offensive_data_counts = offensive_data['label'].value_counts()\n",
        "\n",
        "# # Display label counts\n",
        "# print(offensive_data_counts)\n",
        "\n",
        "# print(offensive_data.head())\n",
        "\n",
        "# # Save the final DataFrame to a CSV file\n",
        "# offensive_data.to_csv(\"final_offensive_data.csv\", index=False)\n",
        "\n",
        "# print(\"merge IEEE_annotated-Oﬀensive_tweets---------------------------------\")\n",
        "\n",
        "# import pandas as pd\n",
        "\n",
        "# # Load the first dataset (final_data)\n",
        "# final_data_df = pd.read_csv(\"/kaggle/working/final_data.csv\")\n",
        "\n",
        "# # Load the second dataset (offensive_data)\n",
        "# offensive_data_df = pd.read_csv(\"/kaggle/working/final_offensive_data.csv\")\n",
        "\n",
        "# # Rename columns in offensive_data\n",
        "# offensive_data_df = offensive_data_df[offensive_data_df['label'] == \"threat: 'Offensive_Language'\"].head(500)\n",
        "# offensive_data_df = offensive_data_df.rename(columns={\"label\": \"label\", \"text\": \"text\"})\n",
        "# offensive_data_df['label'] = \"threat: 'Offensive_Language'\"\n",
        "\n",
        "# # Concatenate the two dataframes vertically\n",
        "# merged_data = pd.concat([final_data_df, offensive_data_df], ignore_index=True)\n",
        "\n",
        "\n",
        "# # Save the final DataFrame to a CSV file\n",
        "# merged_data.to_csv(\"merge_IEEE_annotated_Oﬀensive_tweets.csv\", index=False)\n",
        "\n",
        "# # Count occurrences of each label\n",
        "# label_counts = merged_data['label'].value_counts()\n",
        "\n",
        "# # Display label counts\n",
        "# print(label_counts)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T10:04:17.365881Z",
          "iopub.execute_input": "2024-04-23T10:04:17.366211Z",
          "iopub.status.idle": "2024-04-23T10:04:18.217335Z",
          "shell.execute_reply.started": "2024-04-23T10:04:17.366188Z",
          "shell.execute_reply": "2024-04-23T10:04:18.216465Z"
        },
        "trusted": true,
        "id": "zaO31CO-9G_2",
        "outputId": "06e2b19b-66fe-4f5d-a894-44b422f45570"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "IEEE_annotated---------------------------------\nlabel\nthreat        7563\nirrelevant    6533\nunknown       4156\nName: count, dtype: int64\n                                                 text       label\n4   Data leak from Huazhu Hotels may affect 130 mi...      threat\n6   (good slides): \\n\\nThe Advanced Exploitation o...      threat\n7   CVE-2018-1000532 (beep)\\nhttps://t.co/CaKbo38U...      threat\n8   Will upload some of yesterday's videos which d...      threat\n10  You can’t get to courage without walking throu...  irrelevant\nOﬀensive_tweets---------------------------------\nis_off\n0    11772\n1     1008\nName: count, dtype: int64\nlabel\nthreat    500\nName: count, dtype: int64\n                                                  text   label\n1    Great to see #Qatar reject #China #Xinjiang wh...  threat\n235  @karaswisher: He really is vile. \\n\\nhttps://t...  threat\n353  Fascinating uptick in political momentum to co...  threat\n485  @JayCarney: If you can’t compete with lower pr...  threat\n486  @johnolilly: Lotta dumb moves from this admini...  threat\nmerge IEEE_annotated-Oﬀensive_tweets---------------------------------\nlabel\nthreat        8063\nirrelevant    6533\nunknown       4156\nName: count, dtype: int64\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "\n",
        "# Plot using Seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=label_counts.index, y=label_counts.values)\n",
        "plt.title('Label Counts')\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Counts')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# # Plot using Seaborn pie chart\n",
        "# plt.figure(figsize=(8, 8))\n",
        "# plt.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', startangle=140)\n",
        "# plt.title('Label Distribution')\n",
        "# plt.axis('equal')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T04:11:44.326548Z",
          "iopub.execute_input": "2024-04-20T04:11:44.326926Z",
          "iopub.status.idle": "2024-04-20T04:11:44.678309Z",
          "shell.execute_reply.started": "2024-04-20T04:11:44.326896Z",
          "shell.execute_reply": "2024-04-20T04:11:44.677233Z"
        },
        "trusted": true,
        "id": "JyH7B9wv9HAB",
        "outputId": "5d0b576d-6e0d-4983-baad-2c68d4d6cbeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 1000x600 with 1 Axes>",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAJOCAYAAACqS2TfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABZ/0lEQVR4nO3deXRN9/7/8ddJIokpiSkJFQRFFDW1pEXNqcb9VkUNNc/cRJFe062mGtSteShSNYSW29KqKiViCEVMMcfcUorE1OSEkpCc3x9Wzk8ubQnbRp6PtfZazv68z97vbVnHeZ3PHiw2m80mAAAAAADwyDmY3QAAAAAAAM8qQjcAAAAAAAYhdAMAAAAAYBBCNwAAAAAABiF0AwAAAABgEEI3AAAAAAAGIXQDAAAAAGAQQjcAAAAAAAYhdAMAAAAAYBBCNwAAT4lTp07JYrFo/Pjxj2ybMTExslgsiomJeWTbBAAA/x+hGwAAA0VGRspisWjXrl1mt/JI/Pzzz+rdu7dKly4tV1dXubm56dVXX9WUKVN0/fp1s9uTJM2YMUORkZFmtwEAgCTJyewGAADA02HlypV6++235eLiok6dOqlSpUpKS0vT5s2bNWjQIMXHx2vWrFlmt6kZM2aocOHC6tKli9mtAABA6AYAAH/v5MmTatu2rUqWLKn169eraNGi9rHg4GCdOHFCK1euNLFDAACeTJxeDgCAydLS0hQWFqYaNWrI3d1defPmVd26dbVhw4Y/fc+kSZNUsmRJ5c6dW6+99poOHjx4V82RI0fUqlUrFSxYUK6urqpZs6aWL1+erR7Hjh2rq1evas6cOVkCd6ayZcuqf//+9te3bt3SyJEjVaZMGbm4uKhUqVL697//rdTU1Czvs1gsGjFixF3bK1WqVJaZ6szT9Lds2aLQ0FAVKVJEefPm1VtvvaWLFy9meV98fLw2btwoi8Uii8Wi+vXrS5Ju3rypjz76SM8//7xcXV1VqFAh1alTR9HR0dn6OwEA4H4w0w0AgMmsVqtmz56tdu3aqWfPnkpJSdGcOXMUEBCgHTt2qGrVqlnqFyxYoJSUFAUHB+vGjRuaMmWKGjZsqAMHDsjLy0uSFB8fr1dffVXPPfechg4dqrx582rx4sVq0aKFvv32W7311lsP1OMPP/yg0qVL65VXXrmv+h49emj+/Plq1aqV3nvvPW3fvl1jxozR4cOH9d133z3Qvu/Ur18/FShQQB9++KFOnTqlyZMnKyQkRF9//bUkafLkyerXr5/y5cun999/X5LsfycjRozQmDFj1KNHD7388suyWq3atWuXdu/erSZNmmS7JwAA/gqhGwAAkxUoUECnTp2Ss7OzfV3Pnj1VoUIFTZs2TXPmzMlSf+LECR0/flzPPfecJOn1119XrVq19Mknn2jixImSpP79+6tEiRLauXOnXFxcJEn//Oc/VadOHQ0ZMuSBQrfVatXZs2f15ptv3lf9vn37NH/+fPXo0UOff/65fd+enp4aP368NmzYoAYNGtz3/u9UqFAhrVmzRhaLRZKUkZGhqVOnKjk5We7u7mrRooWGDx+uwoULq0OHDlneu3LlSr3xxhtPxHXnAICcg9PLAQAwmaOjoz1wZ2Rk6MqVK7p165Zq1qyp3bt331XfokULe+CWpJdfflm1atXSjz/+KEm6cuWK1q9fr9atWyslJUWXLl3SpUuXdPnyZQUEBOj48eM6e/bsffdntVolSfnz57+v+sw+QkNDs6x/7733JOmhrv3u1auXPXBLUt26dZWenq5ff/31b9/r4eGh+Ph4HT9+PNv7BwDgQRG6AQB4AsyfP19VqlSxX2tcpEgRrVy5UsnJyXfVPv/883etK1eunE6dOiXp9ky4zWbTBx98oCJFimRZPvzwQ0nShQsX7rs3Nzc3SVJKSsp91f/6669ycHBQ2bJls6z39vaWh4fHfQXkP1OiRIksrwsUKCBJ+v333//2veHh4UpKSlK5cuVUuXJlDRo0SPv37892LwAA3A9OLwcAwGRffvmlunTpohYtWmjQoEHy9PSUo6OjxowZo59//vmBt5eRkSFJ+te//qWAgIB71vxvIP4rbm5uKlas2D1v1vZX7pyRflDp6en3XO/o6HjP9Tab7W+3Wa9ePf3888/6/vvvtWbNGs2ePVuTJk1SRESEevToke1eAQD4K4RuAABM9s0336h06dJaunRplqCaOSv9v+51evSxY8dUqlQpSVLp0qUlSbly5VLjxo0fSY/NmzfXrFmzFBsbK39//7+sLVmypDIyMnT8+HH5+fnZ1ycmJiopKUklS5a0rytQoICSkpKyvD8tLU3nz5/Pdq9/FfYLFiyorl27qmvXrrp69arq1aunESNGELoBAIbh9HIAAEyWOXt752zt9u3bFRsbe8/6ZcuWZbkme8eOHdq+fbuaNWsmSfL09FT9+vX12Wef3TO83vmIrfs1ePBg5c2bVz169FBiYuJd4z///LOmTJkiSXrjjTck3b6T+J0yb/IWGBhoX1emTBlt2rQpS92sWbP+dKb7fuTNm/euIC9Jly9fzvI6X758Klu27F2PMQMA4FFiphsAgMdg7ty5Wr169V3r+/fvr+bNm2vp0qV66623FBgYqJMnTyoiIkIVK1bU1atX73pP2bJlVadOHfXt21epqamaPHmyChUqpMGDB9trpk+frjp16qhy5crq2bOnSpcurcTERMXGxuq3337Tvn37Hqj/MmXKaNGiRWrTpo38/PzUqVMnVapUSWlpadq6dauWLFlif672iy++qM6dO2vWrFlKSkrSa6+9ph07dmj+/Plq0aJFljuX9+jRQ3369FFQUJCaNGmiffv2KSoqSoULF36g/u5Uo0YNzZw5U6NGjVLZsmXl6emphg0bqmLFiqpfv75q1KihggULateuXfrmm28UEhKS7X0BAPB3CN0AADwGM2fOvOf6Ll26qEuXLkpISNBnn32mqKgoVaxYUV9++aWWLFmimJiYu97TqVMnOTg4aPLkybpw4YJefvllffrppypatKi9pmLFitq1a5c++ugjRUZG6vLly/L09FS1atUUFhaWrWP4v//7P+3fv1/jxo3T999/r5kzZ8rFxUVVqlTRhAkT1LNnT3vt7NmzVbp0aUVGRuq7776Tt7e3hg0bdtcp8z179tTJkyc1Z84crV69WnXr1lV0dLQaNWqUrR4lKSwsTL/++qvGjh2rlJQUvfbaa2rYsKHeffddLV++XGvWrFFqaqpKliypUaNGadCgQdneFwAAf8diu587jwAAAAAAgAfGNd0AAAAAABiE0A0AAAAAgEEI3QAAAAAAGITQDQAAAACAQQjdAAAAAAAYhNANAAAAAIBBeE73fcjIyNC5c+eUP39+WSwWs9sBAAAAAJjMZrMpJSVFxYoVk4PDn89nE7rvw7lz5+Tj42N2GwAAAACAJ8yZM2dUvHjxPx0ndN+H/PnzS7r9l+nm5mZyNwAAAAAAs1mtVvn4+Njz4p8hdN+HzFPK3dzcCN0AAAAAALu/uwSZG6kBAAAAAGAQQjcAAAAAAAYhdAMAAAAAYBBCNwAAAAAABiF0AwAAAABgEEI3AAAAAAAGIXQDAAAAAGAQQjcAAAAAAAYhdAMAAAAAYBBCNwAAAAAABiF0AwAAAABgEEI3AAAAAAAGIXQDAAAAAGAQQjcAAAAAAAYhdAMAAAAAYBBCNwAAAAAABiF0AwAAAABgEEI3AAAAAAAGMTV0p6en64MPPpCvr69y586tMmXKaOTIkbLZbPYam82msLAwFS1aVLlz51bjxo11/PjxLNu5cuWK2rdvLzc3N3l4eKh79+66evVqlpr9+/erbt26cnV1lY+Pj8aOHftYjhEAAAAAkHM5mbnzTz75RDNnztT8+fP1wgsvaNeuXeratavc3d317rvvSpLGjh2rqVOnav78+fL19dUHH3yggIAAHTp0SK6urpKk9u3b6/z584qOjtbNmzfVtWtX9erVS4sWLZIkWa1WNW3aVI0bN1ZERIQOHDigbt26ycPDQ7169TLt+O9XjUELzG4BwEOIG9fJ7BYAAABgElND99atW/Xmm28qMDBQklSqVCn997//1Y4dOyTdnuWePHmyhg8frjfffFOStGDBAnl5eWnZsmVq27atDh8+rNWrV2vnzp2qWbOmJGnatGl64403NH78eBUrVkwLFy5UWlqa5s6dK2dnZ73wwgvau3evJk6c+FSEbgAAAADA08nU08tfeeUVrVu3TseOHZMk7du3T5s3b1azZs0kSSdPnlRCQoIaN25sf4+7u7tq1aql2NhYSVJsbKw8PDzsgVuSGjduLAcHB23fvt1eU69ePTk7O9trAgICdPToUf3++++GHycAAAAAIGcydaZ76NChslqtqlChghwdHZWenq7Ro0erffv2kqSEhARJkpeXV5b3eXl52ccSEhLk6emZZdzJyUkFCxbMUuPr63vXNjLHChQokGUsNTVVqamp9tdWq/VhDxUAAAAAkAOZOtO9ePFiLVy4UIsWLdLu3bs1f/58jR8/XvPnzzezLY0ZM0bu7u72xcfHx9R+AAAAAABPJ1ND96BBgzR06FC1bdtWlStXVseOHTVw4ECNGTNGkuTt7S1JSkxMzPK+xMRE+5i3t7cuXLiQZfzWrVu6cuVKlpp7bePOfdxp2LBhSk5Oti9nzpx5BEcLAAAAAMhpTA3df/zxhxwcsrbg6OiojIwMSZKvr6+8vb21bt06+7jVatX27dvl7+8vSfL391dSUpLi4uLsNevXr1dGRoZq1aplr9m0aZNu3rxpr4mOjlb58uXvOrVcklxcXOTm5pZlAQAAAADgQZkauv/xj39o9OjRWrlypU6dOqXvvvtOEydO1FtvvSVJslgsGjBggEaNGqXly5frwIED6tSpk4oVK6YWLVpIkvz8/PT666+rZ8+e2rFjh7Zs2aKQkBC1bdtWxYoVkyS98847cnZ2Vvfu3RUfH6+vv/5aU6ZMUWhoqFmHDgAAAADIAUy9kdq0adP0wQcf6J///KcuXLigYsWKqXfv3goLC7PXDB48WNeuXVOvXr2UlJSkOnXqaPXq1fZndEvSwoULFRISokaNGsnBwUFBQUGaOnWqfdzd3V1r1qxRcHCwatSoocKFCyssLIzHhQEAAAAADGWx2Ww2s5t40lmtVrm7uys5OdmUU81rDFrw2PcJ4NGJG9fJ7BYAAADwiN1vTjT19HIAAAAAAJ5lhG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIIRuAAAAAAAMQugGAAAAAMAghG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIIRuAAAAAAAMQugGAAAAAMAghG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIIRuAAAAAAAMQugGAAAAAMAghG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIIRuAAAAAAAMQugGAAAAAMAghG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIIRuAAAAAAAMQugGAAAAAMAghG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIIRuAAAAAAAMQugGAAAAAMAghG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIIRuAAAAAAAMQugGAAAAAMAghG4AAAAAAAxiauguVaqULBbLXUtwcLAk6caNGwoODlahQoWUL18+BQUFKTExMcs2Tp8+rcDAQOXJk0eenp4aNGiQbt26laUmJiZG1atXl4uLi8qWLavIyMjHdYgAAAAAgBzM1NC9c+dOnT9/3r5ER0dLkt5++21J0sCBA/XDDz9oyZIl2rhxo86dO6eWLVva35+enq7AwEClpaVp69atmj9/viIjIxUWFmavOXnypAIDA9WgQQPt3btXAwYMUI8ePRQVFfV4DxYAAAAAkONYbDabzewmMg0YMEArVqzQ8ePHZbVaVaRIES1atEitWrWSJB05ckR+fn6KjY1V7dq1tWrVKjVv3lznzp2Tl5eXJCkiIkJDhgzRxYsX5ezsrCFDhmjlypU6ePCgfT9t27ZVUlKSVq9efV99Wa1Wubu7Kzk5WW5ubo/+wP9GjUELHvs+ATw6ceM6md0CAAAAHrH7zYlPzDXdaWlp+vLLL9WtWzdZLBbFxcXp5s2baty4sb2mQoUKKlGihGJjYyVJsbGxqly5sj1wS1JAQICsVqvi4+PtNXduI7Mmcxv3kpqaKqvVmmUBAAAAAOBBPTGhe9myZUpKSlKXLl0kSQkJCXJ2dpaHh0eWOi8vLyUkJNhr7gzcmeOZY39VY7Vadf369Xv2MmbMGLm7u9sXHx+fhz08AAAAAEAO9MSE7jlz5qhZs2YqVqyY2a1o2LBhSk5Oti9nzpwxuyUAAAAAwFPIyewGJOnXX3/V2rVrtXTpUvs6b29vpaWlKSkpKctsd2Jiory9ve01O3bsyLKtzLub31nzv3c8T0xMlJubm3Lnzn3PflxcXOTi4vLQxwUAAAAAyNmeiJnuefPmydPTU4GBgfZ1NWrUUK5cubRu3Tr7uqNHj+r06dPy9/eXJPn7++vAgQO6cOGCvSY6Olpubm6qWLGivebObWTWZG4DAAAAAACjmB66MzIyNG/ePHXu3FlOTv9/4t3d3V3du3dXaGioNmzYoLi4OHXt2lX+/v6qXbu2JKlp06aqWLGiOnbsqH379ikqKkrDhw9XcHCwfaa6T58++uWXXzR48GAdOXJEM2bM0OLFizVw4EBTjhcAAAAAkHOYfnr52rVrdfr0aXXr1u2usUmTJsnBwUFBQUFKTU1VQECAZsyYYR93dHTUihUr1LdvX/n7+ytv3rzq3LmzwsPD7TW+vr5auXKlBg4cqClTpqh48eKaPXu2AgICHsvxAQAAAAByrifqOd1PKp7TDeBh8JxuAACAZ89T95xuAAAAAACeNYRuAAAAAAAMQugGAAAAAMAghG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIIRuAAAAAAAMQugGAAAAAMAghG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIIRuAAAAAAAMQugGAAAAAMAghG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIE5mNwAAeLacDq9sdgsAHlKJsANmtwAAzwxmugEAAAAAMAihGwAAAAAAgxC6AQAAAAAwCKEbAAAAAACDELoBAAAAADAIoRsAAAAAAIMQugEAAAAAMAihGwAAAAAAgxC6AQAAAAAwCKEbAAAAAACDELoBAAAAADAIoRsAAAAAAIMQugEAAAAAMAihGwAAAAAAgxC6AQAAAAAwCKEbAAAAAACDELoBAAAAADAIoRsAAAAAAIMQugEAAAAAMAihGwAAAAAAgxC6AQAAAAAwCKEbAAAAAACDELoBAAAAADCI6aH77Nmz6tChgwoVKqTcuXOrcuXK2rVrl33cZrMpLCxMRYsWVe7cudW4cWMdP348yzauXLmi9u3by83NTR4eHurevbuuXr2apWb//v2qW7euXF1d5ePjo7Fjxz6W4wMAAAAA5Fymhu7ff/9dr776qnLlyqVVq1bp0KFDmjBhggoUKGCvGTt2rKZOnaqIiAht375defPmVUBAgG7cuGGvad++veLj4xUdHa0VK1Zo06ZN6tWrl33carWqadOmKlmypOLi4jRu3DiNGDFCs2bNeqzHCwAAAADIWZzM3Pknn3wiHx8fzZs3z77O19fX/mebzabJkydr+PDhevPNNyVJCxYskJeXl5YtW6a2bdvq8OHDWr16tXbu3KmaNWtKkqZNm6Y33nhD48ePV7FixbRw4UKlpaVp7ty5cnZ21gsvvKC9e/dq4sSJWcI5AAAAAACPkqkz3cuXL1fNmjX19ttvy9PTU9WqVdPnn39uHz958qQSEhLUuHFj+zp3d3fVqlVLsbGxkqTY2Fh5eHjYA7ckNW7cWA4ODtq+fbu9pl69enJ2drbXBAQE6OjRo/r999+NPkwAAAAAQA5lauj+5ZdfNHPmTD3//POKiopS37599e6772r+/PmSpISEBEmSl5dXlvd5eXnZxxISEuTp6Zll3MnJSQULFsxSc69t3LmPO6WmpspqtWZZAAAAAAB4UKaeXp6RkaGaNWvq448/liRVq1ZNBw8eVEREhDp37mxaX2PGjNFHH31k2v4BAAAAAM8GU2e6ixYtqooVK2ZZ5+fnp9OnT0uSvL29JUmJiYlZahITE+1j3t7eunDhQpbxW7du6cqVK1lq7rWNO/dxp2HDhik5Odm+nDlzJruHCAAAAADIwUwN3a+++qqOHj2aZd2xY8dUsmRJSbdvqubt7a1169bZx61Wq7Zv3y5/f39Jkr+/v5KSkhQXF2evWb9+vTIyMlSrVi17zaZNm3Tz5k17TXR0tMqXL5/lTumZXFxc5ObmlmUBAAAAAOBBmRq6Bw4cqG3btunjjz/WiRMntGjRIs2aNUvBwcGSJIvFogEDBmjUqFFavny5Dhw4oE6dOqlYsWJq0aKFpNsz46+//rp69uypHTt2aMuWLQoJCVHbtm1VrFgxSdI777wjZ2dnde/eXfHx8fr66681ZcoUhYaGmnXoAAAAAIAcwNRrul966SV99913GjZsmMLDw+Xr66vJkyerffv29prBgwfr2rVr6tWrl5KSklSnTh2tXr1arq6u9pqFCxcqJCREjRo1koODg4KCgjR16lT7uLu7u9asWaPg4GDVqFFDhQsXVlhYGI8LAwAAAAAYymKz2WxmN/Gks1qtcnd3V3JysimnmtcYtOCx7xPAoxM3rpPZLTxWp8Mrm90CgIdUIuyA2S0AwBPvfnOiqaeXAwAAAADwLCN0AwAAAABgEEI3AAAAAAAGIXQDAAAAAGAQQjcAAAAAAAYhdAMAAAAAYBBCNwAAAAAABiF0AwAAAABgEEI3AAAAAAAGIXQDAAAAAGAQQjcAAAAAAAYhdAMAAAAAYBBCNwAAAAAABiF0AwAAAABgEEI3AAAAAAAGIXQDAAAAAGAQQjcAAAAAAAYhdAMAAAAAYBBCNwAAAAAABiF0AwAAAABgEEI3AAAAAAAGIXQDAAAAAGAQQjcAAAAAAAYhdAMAAAAAYBBCNwAAAAAABiF0AwAAAABgEEI3AAAAAAAGIXQDAAAAAGAQQjcAAAAAAAYhdAMAAAAAYBBCNwAAAAAABiF0AwAAAABgEEI3AAAAAAAGIXQDAAAAAGAQQjcAAAAAAAYhdAMAAAAAYBBCNwAAAAAABiF0AwAAAABgEEI3AAAAAAAGIXQDAAAAAGAQQjcAAAAAAAYhdAMAAAAAYBBCNwAAAAAABiF0AwAAAABgEFND94gRI2SxWLIsFSpUsI/fuHFDwcHBKlSokPLly6egoCAlJiZm2cbp06cVGBioPHnyyNPTU4MGDdKtW7ey1MTExKh69epycXFR2bJlFRkZ+TgODwAAAACQw5k+0/3CCy/o/Pnz9mXz5s32sYEDB+qHH37QkiVLtHHjRp07d04tW7a0j6enpyswMFBpaWnaunWr5s+fr8jISIWFhdlrTp48qcDAQDVo0EB79+7VgAED1KNHD0VFRT3W4wQAAAAA5DxOpjfg5CRvb++71icnJ2vOnDlatGiRGjZsKEmaN2+e/Pz8tG3bNtWuXVtr1qzRoUOHtHbtWnl5ealq1aoaOXKkhgwZohEjRsjZ2VkRERHy9fXVhAkTJEl+fn7avHmzJk2apICAgMd6rAAAAACAnMX0me7jx4+rWLFiKl26tNq3b6/Tp09LkuLi4nTz5k01btzYXluhQgWVKFFCsbGxkqTY2FhVrlxZXl5e9pqAgABZrVbFx8fba+7cRmZN5jbuJTU1VVarNcsCAAAAAMCDMjV016pVS5GRkVq9erVmzpypkydPqm7dukpJSVFCQoKcnZ3l4eGR5T1eXl5KSEiQJCUkJGQJ3JnjmWN/VWO1WnX9+vV79jVmzBi5u7vbFx8fn0dxuAAAAACAHMbU08ubNWtm/3OVKlVUq1YtlSxZUosXL1bu3LlN62vYsGEKDQ21v7ZarQRvAAAAAMADM/308jt5eHioXLlyOnHihLy9vZWWlqakpKQsNYmJifZrwL29ve+6m3nm67+rcXNz+9Ng7+LiIjc3tywLAAAAAAAP6okK3VevXtXPP/+sokWLqkaNGsqVK5fWrVtnHz969KhOnz4tf39/SZK/v78OHDigCxcu2Guio6Pl5uamihUr2mvu3EZmTeY2AAAAAAAwiqmh+1//+pc2btyoU6dOaevWrXrrrbfk6Oiodu3ayd3dXd27d1doaKg2bNiguLg4de3aVf7+/qpdu7YkqWnTpqpYsaI6duyoffv2KSoqSsOHD1dwcLBcXFwkSX369NEvv/yiwYMH68iRI5oxY4YWL16sgQMHmnnoAAAAAIAcwNRrun/77Te1a9dOly9fVpEiRVSnTh1t27ZNRYoUkSRNmjRJDg4OCgoKUmpqqgICAjRjxgz7+x0dHbVixQr17dtX/v7+yps3rzp37qzw8HB7ja+vr1auXKmBAwdqypQpKl68uGbPns3jwgAAAAAAhrPYbDab2U086axWq9zd3ZWcnGzK9d01Bi147PsE8OjEjetkdguP1enwyma3AOAhlQg7YHYLAPDEu9+c+ERd0w0AAAAAwLOE0A0AAAAAgEEI3QAAAAAAGITQDQAAAACAQQjdAAAAAAAYhNANAAAAAIBBCN0AAAAAABiE0A0AAAAAgEEI3QAAAAAAGITQDQAAAACAQQjdAAAAAAAYhNANAAAAAIBBCN0AAAAAABgkW6F79+7dOnDggP31999/rxYtWujf//630tLSHllzAAAAAAA8zbIVunv37q1jx45Jkn755Re1bdtWefLk0ZIlSzR48OBH2iAAAAAAAE+rbIXuY8eOqWrVqpKkJUuWqF69elq0aJEiIyP17bffPsr+AAAAAAB4amUrdNtsNmVkZEiS1q5dqzfeeEOS5OPjo0uXLj267gAAAAAAeIplK3TXrFlTo0aN0hdffKGNGzcqMDBQknTy5El5eXk90gYBAAAAAHhaZSt0T5o0Sbt371ZISIjef/99lS1bVpL0zTff6JVXXnmkDQIAAAAA8LRyys6bXnzxxSx3L880btw4OTlla5MAAAAAADxzsjXTXbp0aV2+fPmu9Tdu3FC5cuUeuikAAAAAAJ4F2Qrdp06dUnp6+l3rU1NT9dtvvz10UwAAAAAAPAse6Fzw5cuX2/8cFRUld3d3++v09HStW7dOvr6+j647AAAAAACeYg8Uulu0aCFJslgs6ty5c5axXLlyqVSpUpowYcIjaw4AAAAAgKfZA4XuzGdz+/r6aufOnSpcuLAhTQEAAAAA8CzI1q3GT548+aj7AAAAAADgmZPt53utW7dO69at04ULF+wz4Jnmzp370I0BAAAAAPC0y1bo/uijjxQeHq6aNWuqaNGislgsj7ovAAAAAACeetkK3REREYqMjFTHjh0fdT8AAAAAADwzsvWc7rS0NL3yyiuPuhcAAAAAAJ4p2QrdPXr00KJFix51LwAAAAAAPFOydXr5jRs3NGvWLK1du1ZVqlRRrly5soxPnDjxkTQHAAAAAMDTLFuhe//+/apataok6eDBg1nGuKkaAAAAAAC3ZSt0b9iw4VH3AQAAAADAMydb13QDAAAAAIC/l62Z7gYNGvzlaeTr16/PdkMAAAAAADwrshW6M6/nznTz5k3t3btXBw8eVOfOnR9FXwAAAAAAPPWyFbonTZp0z/UjRozQ1atXH6ohAAAAAACeFY/0mu4OHTpo7ty5j3KTAAAAAAA8tbI10/1nYmNj5erq+ig3CQAAABjq1Wmvmt0CgIe0pd8Ws1v4U9kK3S1btszy2maz6fz589q1a5c++OCDR9IYAAAAAABPu2yFbnd39yyvHRwcVL58eYWHh6tp06aPpDEAAAAAAJ522Qrd8+bNe9R9AAAAAADwzHmoG6nFxcXpyy+/1Jdffqk9e/Y8VCP/+c9/ZLFYNGDAAPu6GzduKDg4WIUKFVK+fPkUFBSkxMTELO87ffq0AgMDlSdPHnl6emrQoEG6detWlpqYmBhVr15dLi4uKlu2rCIjIx+qVwAAAAAA7ke2ZrovXLigtm3bKiYmRh4eHpKkpKQkNWjQQF999ZWKFCnyQNvbuXOnPvvsM1WpUiXL+oEDB2rlypVasmSJ3N3dFRISopYtW2rLltsXyaenpyswMFDe3t7aunWrzp8/r06dOilXrlz6+OOPJUknT55UYGCg+vTpo4ULF2rdunXq0aOHihYtqoCAgOwcPgAAAAAA9yVbM939+vVTSkqK4uPjdeXKFV25ckUHDx6U1WrVu++++0Dbunr1qtq3b6/PP/9cBQoUsK9PTk7WnDlzNHHiRDVs2FA1atTQvHnztHXrVm3btk2StGbNGh06dEhffvmlqlatqmbNmmnkyJGaPn260tLSJEkRERHy9fXVhAkT5Ofnp5CQELVq1epPnzUOAAAAAMCjkq3QvXr1as2YMUN+fn72dRUrVtT06dO1atWqB9pWcHCwAgMD1bhx4yzr4+LidPPmzSzrK1SooBIlSig2NlbS7UeUVa5cWV5eXvaagIAAWa1WxcfH22v+d9sBAQH2bQAAAAAAYJRsnV6ekZGhXLly3bU+V65cysjIuO/tfPXVV9q9e7d27tx511hCQoKcnZ3tp69n8vLyUkJCgr3mzsCdOZ459lc1VqtV169fV+7cue/ad2pqqlJTU+2vrVbrfR8TAAAAAACZsjXT3bBhQ/Xv31/nzp2zrzt79qwGDhyoRo0a3dc2zpw5o/79+2vhwoVydXXNThuGGTNmjNzd3e2Lj4+P2S0BAAAAAJ5C2Qrdn376qaxWq0qVKqUyZcqoTJky8vX1ldVq1bRp0+5rG3Fxcbpw4YKqV68uJycnOTk5aePGjZo6daqcnJzk5eWltLQ0JSUlZXlfYmKivL29JUne3t533c088/Xf1bi5ud1zlluShg0bpuTkZPty5syZ+zomAAAAAADulK3Ty318fLR7926tXbtWR44ckST5+fndde30X2nUqJEOHDiQZV3Xrl1VoUIFDRkyRD4+PsqVK5fWrVunoKAgSdLRo0d1+vRp+fv7S5L8/f01evRoXbhwQZ6enpKk6Ohoubm5qWLFivaaH3/8Mct+oqOj7du4FxcXF7m4uNz3sQAAAAAAcC8PFLrXr1+vkJAQbdu2TW5ubmrSpImaNGki6fbdxl944QVFRESobt26f7ut/Pnzq1KlSlnW5c2bV4UKFbKv7969u0JDQ1WwYEG5ubmpX79+8vf3V+3atSVJTZs2VcWKFdWxY0eNHTtWCQkJGj58uIKDg+2huU+fPvr00081ePBgdevWTevXr9fixYu1cuXKBzl0AAAAAAAe2AOdXj558mT17NlTbm5ud425u7urd+/emjhx4iNrbtKkSWrevLmCgoJUr149eXt7a+nSpfZxR0dHrVixQo6OjvL391eHDh3UqVMnhYeH22t8fX21cuVKRUdH68UXX9SECRM0e/ZsntENAAAAADDcA81079u3T5988smfjjdt2lTjx4/PdjMxMTFZXru6umr69OmaPn36n76nZMmSd50+/r/q16+vPXv2ZLsvAAAAAACy44FmuhMTE+/5qLBMTk5Ounjx4kM3BQAAAADAs+CBQvdzzz2ngwcP/un4/v37VbRo0YduCgAAAACAZ8EDhe433nhDH3zwgW7cuHHX2PXr1/Xhhx+qefPmj6w5AAAAAACeZg90Tffw4cO1dOlSlStXTiEhISpfvrwk6ciRI5o+fbrS09P1/vvvG9IoAAAAAABPmwcK3V5eXtq6dav69u2rYcOGyWazSZIsFosCAgI0ffp0eXl5GdIoAAAAAABPmwcK3dL/v1v477//rhMnTshms+n5559XgQIFjOgPAAAAAICn1gOH7kwFChTQSy+99Ch7AQAAAADgmfJAN1IDAAAAAAD3j9ANAAAAAIBBCN0AAAAAABiE0A0AAAAAgEEI3QAAAAAAGITQDQAAAACAQQjdAAAAAAAYhNANAAAAAIBBCN0AAAAAABiE0A0AAAAAgEEI3QAAAAAAGITQDQAAAACAQQjdAAAAAAAYhNANAAAAAIBBCN0AAAAAABiE0A0AAAAAgEEI3QAAAAAAGITQDQAAAACAQQjdAAAAAAAYhNANAAAAAIBBCN0AAAAAABiE0A0AAAAAgEEI3QAAAAAAGITQDQAAAACAQQjdAAAAAAAYhNANAAAAAIBBCN0AAAAAABiE0A0AAAAAgEEI3QAAAAAAGITQDQAAAACAQQjdAAAAAAAYhNANAAAAAIBBCN0AAAAAABiE0A0AAAAAgEEI3QAAAAAAGITQDQAAAACAQQjdAAAAAAAYxNTQPXPmTFWpUkVubm5yc3OTv7+/Vq1aZR+/ceOGgoODVahQIeXLl09BQUFKTEzMso3Tp08rMDBQefLkkaenpwYNGqRbt25lqYmJiVH16tXl4uKismXLKjIy8nEcHgAAAAAghzM1dBcvXlz/+c9/FBcXp127dqlhw4Z68803FR8fL0kaOHCgfvjhBy1ZskQbN27UuXPn1LJlS/v709PTFRgYqLS0NG3dulXz589XZGSkwsLC7DUnT55UYGCgGjRooL1792rAgAHq0aOHoqKiHvvxAgAAAAByFovNZrOZ3cSdChYsqHHjxqlVq1YqUqSIFi1apFatWkmSjhw5Ij8/P8XGxqp27dpatWqVmjdvrnPnzsnLy0uSFBERoSFDhujixYtydnbWkCFDtHLlSh08eNC+j7Zt2yopKUmrV6++r56sVqvc3d2VnJwsNze3R3/Qf6PGoAWPfZ8AHp24cZ3MbuGxOh1e2ewWADykEmEHzG7hsXp12qtmtwDgIW3pt+Wx7/N+c+ITc013enq6vvrqK127dk3+/v6Ki4vTzZs31bhxY3tNhQoVVKJECcXGxkqSYmNjVblyZXvglqSAgABZrVb7bHlsbGyWbWTWZG7jXlJTU2W1WrMsAAAAAAA8KNND94EDB5QvXz65uLioT58++u6771SxYkUlJCTI2dlZHh4eWeq9vLyUkJAgSUpISMgSuDPHM8f+qsZqter69ev37GnMmDFyd3e3Lz4+Po/iUAEAAAAAOYzpobt8+fLau3evtm/frr59+6pz5846dOiQqT0NGzZMycnJ9uXMmTOm9gMAAAAAeDo5md2As7OzypYtK0mqUaOGdu7cqSlTpqhNmzZKS0tTUlJSltnuxMREeXt7S5K8vb21Y8eOLNvLvLv5nTX/e8fzxMREubm5KXfu3PfsycXFRS4uLo/k+AAAAAAAOZfpM93/KyMjQ6mpqapRo4Zy5cqldevW2ceOHj2q06dPy9/fX5Lk7++vAwcO6MKFC/aa6Ohoubm5qWLFivaaO7eRWZO5DQAAAAAAjGLqTPewYcPUrFkzlShRQikpKVq0aJFiYmIUFRUld3d3de/eXaGhoSpYsKDc3NzUr18/+fv7q3bt2pKkpk2bqmLFiurYsaPGjh2rhIQEDR8+XMHBwfaZ6j59+ujTTz/V4MGD1a1bN61fv16LFy/WypUrzTx0AAAAAEAOYGrovnDhgjp16qTz58/L3d1dVapUUVRUlJo0aSJJmjRpkhwcHBQUFKTU1FQFBARoxowZ9vc7OjpqxYoV6tu3r/z9/ZU3b1517txZ4eHh9hpfX1+tXLlSAwcO1JQpU1S8eHHNnj1bAQEBj/14AQAAAAA5i6mhe86cOX857urqqunTp2v69Ol/WlOyZEn9+OOPf7md+vXra8+ePdnqEQAAAACA7HrirukGAAAAAOBZQegGAAAAAMAghG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIIRuAAAAAAAMQugGAAAAAMAghG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIIRuAAAAAAAMQugGAAAAAMAghG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIIRuAAAAAAAMQugGAAAAAMAghG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIIRuAAAAAAAMQugGAAAAAMAghG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIIRuAAAAAAAMQugGAAAAAMAghG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIIRuAAAAAAAMQugGAAAAAMAghG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIIRuAAAAAAAMYmroHjNmjF566SXlz59fnp6eatGihY4ePZql5saNGwoODlahQoWUL18+BQUFKTExMUvN6dOnFRgYqDx58sjT01ODBg3SrVu3stTExMSoevXqcnFxUdmyZRUZGWn04QEAAAAAcjhTQ/fGjRsVHBysbdu2KTo6Wjdv3lTTpk117do1e83AgQP1ww8/aMmSJdq4caPOnTunli1b2sfT09MVGBiotLQ0bd26VfPnz1dkZKTCwsLsNSdPnlRgYKAaNGigvXv3asCAAerRo4eioqIe6/ECAAAAAHIWJzN3vnr16iyvIyMj5enpqbi4ONWrV0/JycmaM2eOFi1apIYNG0qS5s2bJz8/P23btk21a9fWmjVrdOjQIa1du1ZeXl6qWrWqRo4cqSFDhmjEiBFydnZWRESEfH19NWHCBEmSn5+fNm/erEmTJikgIOCxHzcAAAAAIGd4oq7pTk5OliQVLFhQkhQXF6ebN2+qcePG9poKFSqoRIkSio2NlSTFxsaqcuXK8vLystcEBATIarUqPj7eXnPnNjJrMrcBAAAAAIARTJ3pvlNGRoYGDBigV199VZUqVZIkJSQkyNnZWR4eHllqvby8lJCQYK+5M3BnjmeO/VWN1WrV9evXlTt37ixjqampSk1Ntb+2Wq0Pf4AAAAAAgBzniZnpDg4O1sGDB/XVV1+Z3YrGjBkjd3d3++Lj42N2SwAAAACAp9ATEbpDQkK0YsUKbdiwQcWLF7ev9/b2VlpampKSkrLUJyYmytvb217zv3czz3z9dzVubm53zXJL0rBhw5ScnGxfzpw589DHCAAAAADIeUwN3TabTSEhIfruu++0fv16+fr6ZhmvUaOGcuXKpXXr1tnXHT16VKdPn5a/v78kyd/fXwcOHNCFCxfsNdHR0XJzc1PFihXtNXduI7Mmcxv/y8XFRW5ublkWAAAAAAAelKnXdAcHB2vRokX6/vvvlT9/fvs12O7u7sqdO7fc3d3VvXt3hYaGqmDBgnJzc1O/fv3k7++v2rVrS5KaNm2qihUrqmPHjho7dqwSEhI0fPhwBQcHy8XFRZLUp08fffrppxo8eLC6deum9evXa/HixVq5cqVpxw4AAAAAePaZOtM9c+ZMJScnq379+ipatKh9+frrr+01kyZNUvPmzRUUFKR69erJ29tbS5cutY87OjpqxYoVcnR0lL+/vzp06KBOnTopPDzcXuPr66uVK1cqOjpaL774oiZMmKDZs2fzuDAAAAAAgKFMnem22Wx/W+Pq6qrp06dr+vTpf1pTsmRJ/fjjj3+5nfr162vPnj0P3CMAAAAAANn1RNxIDQAAAACAZxGhGwAAAAAAgxC6AQAAAAAwCKEbAAAAAACDELoBAAAAADAIoRsAAAAAAIMQugEAAAAAMAihGwAAAAAAgxC6AQAAAAAwCKEbAAAAAACDELoBAAAAADAIoRsAAAAAAIMQugEAAAAAMAihGwAAAAAAgxC6AQAAAAAwCKEbAAAAAACDELoBAAAAADAIoRsAAAAAAIMQugEAAAAAMAihGwAAAAAAgxC6AQAAAAAwCKEbAAAAAACDELoBAAAAADAIoRsAAAAAAIMQugEAAAAAMAihGwAAAAAAgxC6AQAAAAAwCKEbAAAAAACDELoBAAAAADAIoRsAAAAAAIMQugEAAAAAMAihGwAAAAAAgxC6AQAAAAAwCKEbAAAAAACDELoBAAAAADAIoRsAAAAAAIMQugEAAAAAMAihGwAAAAAAgxC6AQAAAAAwCKEbAAAAAACDELoBAAAAADAIoRsAAAAAAIMQugEAAAAAMAihGwAAAAAAg5gaujdt2qR//OMfKlasmCwWi5YtW5Zl3GazKSwsTEWLFlXu3LnVuHFjHT9+PEvNlStX1L59e7m5ucnDw0Pdu3fX1atXs9Ts379fdevWlaurq3x8fDR27FijDw0AAAAAAHND97Vr1/Tiiy9q+vTp9xwfO3aspk6dqoiICG3fvl158+ZVQECAbty4Ya9p37694uPjFR0drRUrVmjTpk3q1auXfdxqtapp06YqWbKk4uLiNG7cOI0YMUKzZs0y/PgAAAAAADmbk5k7b9asmZo1a3bPMZvNpsmTJ2v48OF68803JUkLFiyQl5eXli1bprZt2+rw4cNavXq1du7cqZo1a0qSpk2bpjfeeEPjx49XsWLFtHDhQqWlpWnu3LlydnbWCy+8oL1792rixIlZwjkAAAAAAI/aE3tN98mTJ5WQkKDGjRvb17m7u6tWrVqKjY2VJMXGxsrDw8MeuCWpcePGcnBw0Pbt2+019erVk7Ozs70mICBAR48e1e+//37PfaempspqtWZZAAAAAAB4UE9s6E5ISJAkeXl5ZVnv5eVlH0tISJCnp2eWcScnJxUsWDBLzb22cec+/teYMWPk7u5uX3x8fB7+gAAAAAAAOc4TG7rNNGzYMCUnJ9uXM2fOmN0SAAAAAOAp9MSGbm9vb0lSYmJilvWJiYn2MW9vb124cCHL+K1bt3TlypUsNffaxp37+F8uLi5yc3PLsgAAAAAA8KCe2NDt6+srb29vrVu3zr7OarVq+/bt8vf3lyT5+/srKSlJcXFx9pr169crIyNDtWrVstds2rRJN2/etNdER0erfPnyKlCgwGM6GgAAAABATmRq6L569ar27t2rvXv3Srp987S9e/fq9OnTslgsGjBggEaNGqXly5frwIED6tSpk4oVK6YWLVpIkvz8/PT666+rZ8+e2rFjh7Zs2aKQkBC1bdtWxYoVkyS98847cnZ2Vvfu3RUfH6+vv/5aU6ZMUWhoqElHDQAAAADIKUx9ZNiuXbvUoEED++vMINy5c2dFRkZq8ODBunbtmnr16qWkpCTVqVNHq1evlqurq/09CxcuVEhIiBo1aiQHBwcFBQVp6tSp9nF3d3etWbNGwcHBqlGjhgoXLqywsDAeFwYAAAAAMJypobt+/fqy2Wx/Om6xWBQeHq7w8PA/rSlYsKAWLVr0l/upUqWKfvrpp2z3CQAAAABAdjyx13QDAAAAAPC0I3QDAAAAAGAQQjcAAAAAAAYhdAMAAAAAYBBCNwAAAAAABiF0AwAAAABgEEI3AAAAAAAGIXQDAAAAAGAQQjcAAAAAAAYhdAMAAAAAYBBCNwAAAAAABiF0AwAAAABgEEI3AAAAAAAGIXQDAAAAAGAQQjcAAAAAAAYhdAMAAAAAYBBCNwAAAAAABiF0AwAAAABgEEI3AAAAAAAGIXQDAAAAAGAQQjcAAAAAAAYhdAMAAAAAYBBCNwAAAAAABiF0AwAAAABgEEI3AAAAAAAGIXQDAAAAAGAQQjcAAAAAAAYhdAMAAAAAYBBCNwAAAAAABiF0AwAAAABgEEI3AAAAAAAGIXQDAAAAAGAQQjcAAAAAAAYhdAMAAAAAYBBCNwAAAAAABiF0AwAAAABgEEI3AAAAAAAGIXQDAAAAAGAQQjcAAAAAAAYhdAMAAAAAYBBCNwAAAAAABiF0AwAAAABgEEI3AAAAAAAGyVGhe/r06SpVqpRcXV1Vq1Yt7dixw+yWAAAAAADPsBwTur/++muFhobqww8/1O7du/Xiiy8qICBAFy5cMLs1AAAAAMAzKseE7okTJ6pnz57q2rWrKlasqIiICOXJk0dz5841uzUAAAAAwDMqR4TutLQ0xcXFqXHjxvZ1Dg4Oaty4sWJjY03sDAAAAADwLHMyu4HH4dKlS0pPT5eXl1eW9V5eXjpy5Mhd9ampqUpNTbW/Tk5OliRZrVZjG/0T6anXTdkvgEfDrM8Os6TcSDe7BQAPKad9bt26fsvsFgA8JDM+tzL3abPZ/rIuR4TuBzVmzBh99NFHd6338fExoRsATzv3aX3MbgEAHswYd7M7AIAH4j7EvM+tlJQUubv/+f5zROguXLiwHB0dlZiYmGV9YmKivL2976ofNmyYQkND7a8zMjJ05coVFSpUSBaLxfB+kXNYrVb5+PjozJkzcnNzM7sdAPhbfG4BeNrwuQWj2Gw2paSkqFixYn9ZlyNCt7Ozs2rUqKF169apRYsWkm4H6XXr1ikkJOSuehcXF7m4uGRZ5+Hh8Rg6RU7l5ubGfwIAnip8bgF42vC5BSP81Qx3phwRuiUpNDRUnTt3Vs2aNfXyyy9r8uTJunbtmrp27Wp2awAAAACAZ1SOCd1t2rTRxYsXFRYWpoSEBFWtWlWrV6++6+ZqAAAAAAA8KjkmdEtSSEjIPU8nB8zi4uKiDz/88K7LGQDgScXnFoCnDZ9bMJvF9nf3NwcAAAAAANniYHYDAAAAAAA8qwjdAAAAAAAYhNANAAAAAIBBCN0AAAAAABiE0A08JTIyMpR530PufwgAAAA8HQjdwBMqIyNDknTjxg1JkoODgw4fPixJslgspvUFAA8j87NNkm7duiVJSklJMasdAAAMR+gGnlAODg765ZdfFBwcrFOnTumbb75RpUqVtH//frNbA4Bsc3Bw0IkTJ7R582Y5OTnpm2++0ZAhQ5ScnGx2awAAGILQDTzBLl68qOXLl6tjx47q0KGDIiMjVaVKFU4vB/DUSktL04QJE1SvXj2NHDlSrVu3lr+/v9zd3c1uDQDuie9deFgWG/+KgCeSzWaTxWLRxIkTNWjQINWsWVPz589XhQoVsowDwNMmMTFRQUFBio2N1dChQzV69Gilp6fL0dHR7NYAIIvM71sxMTFauXKlihYtKn9/f/n7+5vdGp4izHQDT7g8efJo1KhRunLlij788EPt2rVL0u3ruu/8zYzfzwA8LfLkySMXFxdVq1ZNs2bNUkxMjBwdHZWens5nGYAnisVi0YoVK/T6668rLi5Os2fPVq9evfTVV1+Z3RqeIsx0A0+YP5vB/umnn9SlSxfVqFFDQ4cOVfXq1e3r69at+7jbBIAH8r+fbVevXtWlS5c0fPhwrVq1St9++63q16+vjIwMOTg46OLFiypSpIiJHQOAdP78ec2bN0+enp7q0aOH9u7dq7lz52rp0qUaO3as3nnnHbNbxFOA0A08QTK/lG7dulX79u3T+fPn9c4776hEiRLKkyePfvrpJ3Xt2lU1a9ZU27ZttX//fo0YMULnzp2Tl5cXp5sDeCJlfrZt375dJ06ckMVisX9RPX78uEaOHKkff/xR33zzjerXr68xY8bo7NmzmjBhglxcXEzuHkBOtX//fr3zzjtycHDQzJkz9eqrr0qSjh07pk8//VRLly7V+PHj1bZtW5M7xZPOyewGANyW+aV06dKl6tatm+rWratjx45pzZo1ateunbp27aq6detq/vz56t+/v0aMGCGr1aqdO3fK29vb7PYB4E9ZLBYtW7ZM7dq1U/ny5RUfH69Fixbps88+0/PPP6+wsDA5OjqqYcOGatq0qdatW6cdO3YQuAE8dpnfx2w2m2w2m8qVK6eoqChduHDBXlOuXDn169dPjo6O6tq1q5ycnNSqVSsTu8aTjplu4AmyefNmtW7dWqNGjVK3bt109uxZlSpVSn5+fmrfvr3++c9/Kn/+/Dp9+rRSUlJUqFAhAjeAJ1bml9eUlBS1bt1a7dq1U/PmzXX27FkFBgbK19dXX3zxhUqUKKHff/9d3333nY4ePapu3bqpfPnyZrcPIIfasWOHYmNj1b9/f+3atUsff/yx9u/frxkzZqhp06b2uiNHjigyMlLdu3fX888/b2LHeNIRuoEnREZGhj7//HMdOnRIU6ZM0S+//KImTZrotdde061btxQVFaXBgwerR48ePFoHwFNj7dq1mjFjhhwdHTVu3DiVKlVKknT69GnVqVNHvr6+WrBggUqWLCmJJzMAMN+AAQP0448/avfu3cqXL5+2bt2q6dOna//+/Zo4caKaNGlir71586Zy5cplYrd4GhC6gSfIyZMndePGDZUqVUpvvPGGSpcurTlz5ujq1asqU6aM8uXLp5CQEA0YMIAvpQCeClu3blWzZs2UlpamHTt2qHLlyvabpZ05c0b169dX/vz5tWzZMpUqVYrQDeCxy/zcuTNAly9fXtWqVbPfpXzr1q369NNPdfjwYY0ePVpvvPGGmS3jKcMjwwCT3Pl7182bNyVJvr6+8vPz0+HDh3XhwgX16tVLkvTbb7/ppZdeUpMmTdSyZUu+kAJ4arzyyitau3at8ubNq/DwcKWkpMjBwUE2m00+Pj5at26d0tPT7Z9rfL4BeNwsFouio6M1evRo7dy5U5I0Y8YMHT58WPPmzZN0+7Osf//+Kl68uEaPHq0//viDRxzivhG6ARNk/qIaFRWl7t2767XXXtPIkSMVGxsr6XYIT01N1fHjx2W1WvX1118rd+7cGj9+vP0UTAB40mR+AU1OTlZKSop9/UsvvaTly5crOjpavXr1UkpKiiwWizIyMlSqVCnt2bOHzzYAprl+/bomTZqk8PBw9erVSzNnzlT16tX18ssva8uWLTp//rwkqVatWvrwww+1ePFi5cmThx8Jcd84vRwwyffff68OHTqoR48eKleunMaPH6+iRYsqMjJSxYsXV5s2bbR//345OjoqKSlJa9assT+bGwCeNJk/Jq5cuVKffPKJUlJS5OzsrM8++0x+fn5ycXHR1q1b9cYbb+gf//iHpk+fLjc3N7PbBpBD3Xkpi81m05IlSzRz5kw1b95ckyZNUtu2beXo6KiIiAh9/vnnat26tckd42nGTDdggoSEBI0ePVoff/yxJk2apF69eik5OVm1a9dW6dKl5erqqq+++kpjx45VWFiYtm/fTuAG8ESzWCz64Ycf1K5dOzVo0EDTpk1Tvnz51L59e0VFRSk1NVWvvPKKVq1apYULFyo0NJRTMwGYxmKxKDY2VsuWLZPFYlHLli2VN29e/frrrzp27JhsNpv++OMPpaSkqEuXLjp8+LDZLeMpxkw38Jjc+YvqlStX1KRJE61evVopKSmqW7euAgMDNWvWLEnShg0b9PLLLytv3rxmtgwA9+3UqVNq27at2rRpo4EDB+rChQuqXbu2bty4odTUVM2bN09NmjRR7ty5tX37dnl4ePBYMACmsVqtev/99zV9+nSFhoYqNDRUuXPnVoMGDRQaGqoOHTpo3759GjlypKKjoxUfH68SJUqY3TaeUoRu4DFasGCBbt68qaCgIL344osaM2aMPvzwQzVo0EAzZ86Uo6Ojjh8/rqFDh6p///6qV6+e2S0DwJ+688fE06dP6+uvv1afPn109epVvfbaa2rUqJFmzpypunXr6sqVKxoxYoT+7//+Ty4uLiZ3DgC3r+WOiYlRSEiIypYtq3r16um5557Tzp079a9//Uu+vr6SpIsXL6pIkSImd4unGaeXAwbL/F3r+PHj6t27ty5evCgPDw+1a9dOHTt2VPny5TVr1iw5OjpKkiIjI/XLL7+oTJkyZrYNAH/LYrFo27Zt2r59u0qUKKG33npL+fPn15gxY1SpUiWNGzdOklSuXDkdOXJEQ4cOtT+tAQAep8zvYydOnFBMTIz279+va9euqVmzZlq9erX8/f21bNkyvfvuu4qOjtaGDRvs7y1cuLBZbeMZwUw38Bjs2LFDP/30ky5evKj//Oc/kqSDBw9q1KhR2rRpkz7++GPZbDbt2bNHkZGR+umnn/Tiiy+a3DUA/DmbzSabzaYqVarIz89PS5YssY/93//9n55//nmNHz9eFotFoaGh6tixo7y9vVW0aFETuwaQE2WelfPtt9/a7yfh5OSkfPnyaf78+apWrZpSUlKUkJCgESNG6L///a+KFy+uY8eOycXFhbuU46E5md0A8Ky7dOmSRo0apbVr16pVq1b29ZUqVdKQIUNUvHhxDR06VM8995yee+45bdmyRZUrVzaxYwC4Pw4ODho3bpxCQkK0bt06NWrUSJKUN29eLVu2TOXKldOePXv01VdfqX///gRuAKbIPCunc+fOGj9+vAICAnT8+HF99tlneu2117R582ZVqVJF+fPn18KFC/X666/L399frq6uZreOZwQz3cBjsHz5ck2fPt0+412pUqUs4xcvXlTBggWVmpqqPHnymNQlADwYm82mM2fOqGvXrqpTp44++ugjSVJaWpr+8Y9/6Ny5c3J2dtacOXNUtWpVc5sFkKPNnDlTy5YtU1RUlH3d2bNnFRISovPnzysqKkr58+eXgwNX3+LR418V8Ailp6fbrxlKTU1VWlqapNunWv7rX/9S9erV1a1bN8XHx2epL1y4sBwdHZU7d27TegeA+7F//36tXbtW0u3ZoxIlSigoKEjjx4/Xzz//LElydnZWVFSU1q9fr5iYGAI3ANOlpKRo7969un79uqTbPxo+99xz6tixoxITE3Xp0iUCNwzDvyzgEdi0aZMkydHRURaLRStWrNCbb76pli1b2q/hbtKkiQYNGqTChQure/fuOnTokBwdHbPc/ZdrhgA8qTIyMpSYmKghQ4YoKChIffv21YYNG2Sz2fTPf/5T9erVU0REhFJTU3Xr1i1JUpEiRZQ/f36TOwcA6ZVXXpG3t7fmzZunq1ev2r9zlStXThaLRVar1eQO8SwjdAMPad++fapfv77ef/99SVJMTIxat26tkiVLqlChQhoxYoS6desmSXr99df17rvvqkiRImrZsqWOHDnCr6oAngoODg7y8vLSnDlz9O233yo2Nlb//ve/FRAQoCNHjqhcuXKKi4vTjRs35OTELWMAmCPzjMN9+/ZpzZo12rhxo6Tbobt69eqKjIzU3LlzlZSUpOvXr+vLL7+Us7OzihcvbmbbeMZxTTfwkFJTU7VgwQK9++67Gjp0qKpXr67jx48rNDRUt27d0rp169S6dWu99dZbioyMlHT7Gu8vv/xSY8eOValSpUztHwD+TOaZOAcOHNCRI0fk5OSkF154QeXKldPly5e1Y8cOTZgwQQkJCSpbtqyWL1+ujz/+WEOHDjW7dQA52HfffacOHTrIx8dHx44dU79+/TR58mRlZGSod+/e2rlzp37++WdVrVpVR44cUXR0tKpVq2Z223iGEbqBbMjIyLhrhvqzzz7TgAEDlD9/fg0bNkwDBw60j0VFRentt9/W22+/rTlz5kiSrl27prx58z7WvgHgQX377bcaMGCAvL29lS9fPu3Zs0f//e9/1axZM3vNl19+qf3792vevHmKiYnRCy+8YGLHAHKizB8JL1++rMDAQPXp00f169fX/v371aZNG7399tuaO3euHB0dFR8fry1btsjDw0Mvv/yyfH19zW4fzzhCN5BNZ86c0bZt2/T2229r8eLF+v7779WoUSOFhobq7bff1ueff56lPjo6WgEBAerTp49mzJhhUtcAcG+ZX1jv/FFx165datq0qcaMGaPevXtr27ZteuWVVzR06FB9/PHHSk9Pl6Ojo30bV69eVb58+cw6BAA5XFRUlKKionTlyhVNnjxZHh4ekm5f+tesWTO1bt1aU6dOlbu7u7mNIsfhoisgG27evKnBgwfr9OnT2rp1q6ZMmaK5c+eqc+fOkqQ+ffrI29tbI0eOtL+nSZMmWrt2rZ577jmz2gaAP7Vjxw7VqlVLDg4O9jB99OhRNWrUSL1799avv/6q1q1bq2/fvvr4448lSZcvX5anp6d9GwRuAGY6f/68Jk+eLE9PT6WkpMjDw0MZGRmqX7++Vq1apTfffFN//PGHPvvsMxUsWNDsdpGDcAcnIBty5cqlmTNnKj09XVOmTFGfPn3UpUsXWSwWvfPOO5o5c6b+85//6IMPPsjyvoYNG6p8+fImdQ0A97Z161b5+/vrk08+kST77PXvv/+uq1ev6tixY6pbt66aNWumadOmSZLWrFmjcePGccdfAKb76aefdPToUXXp0kVLlizR5cuXNWPGDN26dUsODg6y2WyqX7++vvnmG23ZskWpqalmt4wchtANZFPevHmVN29evfjiizpx4oQWLlwoSXJ1ddU777yjiIgITZgwQaGhoSZ3CgB/rUyZMgoPD9fYsWM1btw4+/qyZcvq0qVLqlu3rpo2barPPvvM/pidFStWKCEhgScwAHisrly5Iun2/XVsNpvOnDmjTp066dq1a5KkoKAgzZ07V+PGjVN4eLjS09NlsVhks9nUpEkT/fzzzypatKiZh4AciNPLgWzKlSuXfvzxR/3+++/q0aOH5syZI5vNpg4dOih37tzq3r27rFarPvnkEw0bNkxFihQxu2UAuCcvLy/17t1bDg4OGjVqlCRp0KBBev3117Vo0SLt2bNHjRo10uXLl5Wenq6JEyfqv//9rzZu3Mgp5QAem8WLF6tdu3aKj49XhQoVJEkFChSQk5OT8ubNa780pmPHjrJYLOrSpYscHBw0fPhw+6MMc+fObeYhIIcidAMPwcXFRd7e3po6dareffddRUZGymazqWPHjvrwww/166+/6tChQ1w3BOCJV6RIEXXv3l2SNHLkSN26dUvDhg3TggULdOnSJYWHh6tv376qXLmyzp49qzVr1qhixYomdw0gJ6ldu7aaNGmihg0bav369apQoYKuXLkim80mNzc3+6UxmZMgFotFHTt2VK5cufT++++b3D1yMu5eDjwiJ0+e1Hvvvafjx4/L1dVVx48fV1RUlGrVqmV2awBwT5l3LL9TUlKSpk+frk8++URDhgyxf1HdtGmTTp06pRIlSuj555/nppAATHH27Fn16tVLO3fu1IYNG1SgQAHVrl1be/bsUaFChe6qX7x4sSpXriw/Pz8TugVuI3QDj9DZs2cVFRWl3377TW3atOGmaQCeWJmBe8OGDYqNjdWBAwfUqVMnVatWTYULF9bYsWPtwfvf//632e0CgN1vv/2m3r17a+/evZo9e7ZGjRql6tWrq0aNGnJ0dFRKSopu3rypypUrq2HDhma3CxC6AQDIqZYuXaouXbqoXbt2unLlio4fPy5fX1998cUXun79umbPnq0JEyYoJCREI0aMMLtdADnUvc7KyQzeq1atUpkyZfTCCy/o4sWLkqQbN27IxcVFn3/+uV544QUzWgayIHQDAJAD/fzzzwoMDNR7772nnj17Kjk5Wd7e3goNDdXo0aMlScnJyRo3bpy++OIL7d69WwULFrzriy8AGCkzcO/atUsnTpxQ/vz5FRgYKOl28B46dKh++OEH7dmzR6VLl7bX37hxQ66uriZ3D9xG6AYAIAfI/O8+MzTv27dPHTp00O7du3Xq1Ck1atRIr7/+umbNmiVJ2rlzp6pWraqUlBTZbLZ7XisJAI/DsmXL1KZNG/n5+Wn//v3q0KGDhg8frnLlyuncuXPq2rWrDh06pB9++EFVq1aVdO/ZccAsPFwTAIAcwGKxyGKxaNWqVTp06JCuXbsmFxcX/fbbb2ratKkCAgIUEREhSdq+fbvmzZunkydPqmDBggRuAI9d5g+FFy9e1IwZMxQREaHNmzdry5YtWr16td5//30dPnxYxYoV0/z581WiRAm1bt1aN2/elCQCN54ohG4AAHKI7du3KzAwUAcPHlTt2rWVnp6uMmXKqHnz5vr888/l4HD7a8G3336rAwcOqECBAiZ3DCCnslgsioqK0rBhw1SgQAE1a9ZM+fLlk7+/v3788Udt2rRJH3zwgY4cOSJvb28tXbpUa9euVa5cucxuHbgLp5cDAJADHDhwQCdPntSRI0c0ePBgSdLWrVvVq1cvFS5cWJ9++qkSEhIUFRWlWbNmafPmzapcubLJXQPIaTJPC8/IyNDmzZtVv359ubi4aOvWrapWrVqWa7zfeust+fn56dNPP1W5cuXMbh34U8x0AwDwjDt//ryCgoL01ltv6dq1a/b11atX17Rp02S1WtWkSROFhIQoNjZWGzduJHADMIXFYtFPP/2kPn36qGrVqtq5c6du3rypqVOnKiEhQRaLRTabTTVr1tSSJUv066+/Kk+ePGa3DfwlJ7MbAAAAxlm1apV2796tIUOG6JNPPtH27dvtY66urmrQoIF2796tvXv3qkiRIsqTJw+nlQMw1YkTJ7RixQrlypVLY8eOVUxMjOrXry9nZ2d99NFH8vb2ls1mU+3atbV//365uLiY3TLwl5jpBgDgGXLnVWM7d+7UO++8o/Lly6tly5YKCwvTjh071LlzZ3tNWlqaJKlq1ap67rnnCNwATNe1a1f95z//UXR0tN577z1Vr15dGzZs0Jw5cxQeHq5z587Zb5Tm7OxscrfA3yN0AwDwDPj666915MgR+xfREydOaP369erTp49atWqlAgUKqGXLlpo6darWrl2rrl27Srr9hTU9Pd3M1gFAv/zyi86dO2d/3alTJ/373//Whg0bNGDAANWqVUsxMTGKiIjQuHHj7J9b3KUcTwNOLwcA4Cn322+/6dNPP9WiRYskSb///rvq16+vS5cuZZnVzpMnj9566y1J0vvvv69WrVrpm2++kaOjoyl9A4B0+zOrTp066tq1q0JCQlS0aFFJUpcuXZSenq7evXvLyclJY8eO1datW+Xu7s7nFp4q3L0cAIBnwPXr15U7d24dOHBAxYsX19GjR9WmTRsVLlxYEREReumll+y1165d03//+19NmDBB69atU7FixUzsHACkmJgYde3aVV27dlWPHj2yfC7VrFlTx44dU69evTRu3Dhmt/HUIXQDAPCMsFqtqlOnjipVqqRp06bpxIkTatWqlRo1aqT33nsvyx3J//jjD926dUtubm4mdgwA/9/mzZvVrl079erVSz169FDRokX1xx9/6L333lOJEiXUpk0blS5d2uw2gQdG6AYA4Bmya9cu9e3bV1WqVNH48eN16NAhtWvXzh68K1WqZHaLAPCnNm/erI4dO6p58+Z65ZVXFB8fr++++06xsbH8SIinFqEbAIBnzJ49e9StWzdVr17dHrw7deqkatWqKTw8XBUrVjS7RQD4U7t27VJoaKhOnTql/Pnz64svvlD16tXNbgvINkI3AADPoDuD94QJE7R3717169dPUVFRXMMN4ImXkpKipKQkubq6qkiRIma3AzwUQjcAAM+oPXv2qFevXipdurRmzZolZ2dn5c6d2+y2AADIUXhONwAAz6hq1appxowZSkhI0B9//EHgBgDABMx0AwDwjLtx44ZcXV3NbgMAgByJ0A0AAAAAgEE4vRwAAAAAAIMQugEAAAAAMAihGwAAAAAAgxC6AQAAAAAwCKEbAAAAAACDELoBAAAAADAIoRsAANxTZGSkPDw8Hno7FotFy5Yte+jtAADwNCJ0AwDwDOvSpYtatGhhdhsAAORYhG4AAAAAAAxC6AYAIIeaOHGiKleurLx588rHx0f//Oc/dfXq1bvqli1bpueff16urq4KCAjQmTNnsox///33ql69ulxdXVW6dGl99NFHunXr1j33mZaWppCQEBUtWlSurq4qWbKkxowZY8jxAQDwJCB0AwCQQzk4OGjq1KmKj4/X/PnztX79eg0ePDhLzR9//KHRo0drwYIF2rJli5KSktS2bVv7+E8//aROnTqpf//+OnTokD777DNFRkZq9OjR99zn1KlTtXz5ci1evFhHjx7VwoULVapUKSMPEwAAU1lsNpvN7CYAAIAxunTpoqSkpPu6kdk333yjPn366NKlS5Ju30ita9eu2rZtm2rVqiVJOnLkiPz8/LR9+3a9/PLLaty4sRo1aqRhw4bZt/Pll19q8ODBOnfunKTbN1L77rvv1KJFC7377ruKj4/X2rVrZbFYHv0BAwDwhGGmGwCAHGrt2rVq1KiRnnvuOeXPn18dO3bU5cuX9ccff9hrnJyc9NJLL9lfV6hQQR4eHjp8+LAkad++fQoPD1e+fPnsS8+ePXX+/Pks28nUpUsX7d27V+XLl9e7776rNWvWGH+gAACYiNANAEAOdOrUKTVv3lxVqlTRt99+q7i4OE2fPl3S7euu79fVq1f10Ucfae/evfblwIEDOn78uFxdXe+qr169uk6ePKmRI0fq+vXrat26tVq1avXIjgsAgCeNk9kNAACAxy8uLk4ZGRmaMGGCHBxu/wa/ePHiu+pu3bqlXbt26eWXX5YkHT16VElJSfLz85N0O0QfPXpUZcuWve99u7m5qU2bNmrTpo1atWql119/XVeuXFHBggUfwZEBAPBkIXQDAPCMS05O1t69e7OsK1y4sG7evKlp06bpH//4h7Zs2aKIiIi73psrVy7169dPU6dOlZOTk0JCQlS7dm17CA8LC1Pz5s1VokQJtWrVSg4ODtq3b58OHjyoUaNG3bW9iRMnqmjRoqpWrZocHBy0ZMkSeXt7y8PDw4hDBwDAdJxeDgDAMy4mJkbVqlXLsnzxxReaOHGiPvnkE1WqVEkLFy6856O78uTJoyFDhuidd97Rq6++qnz58unrr7+2jwcEBGjFihVas2aNXnrpJdWuXVuTJk1SyZIl79lL/vz5NXbsWNWsWVMvvfSSTp06pR9//NE+2w4AwLOGu5cDAAAAAGAQflYGAAAAAMAghG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIIRuAAAAAAAMQugGAAAAAMAghG4AAAAAAAxC6AYAAAAAwCCEbgAAAAAADELoBgAAAADAIIRuAAAAAAAM8v8AL8wMOL/MyFYAAAAASUVORK5CYII="
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "m23NQsca9HAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations and imports"
      ],
      "metadata": {
        "id": "MteBEPfq9HAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # for empty the output path\n",
        "# import shutil\n",
        "# shutil.rmtree(\"/kaggle/working/\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "1V6U-ou59HAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \"torch==2.1.2\" tensorboard"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T10:04:26.277812Z",
          "iopub.execute_input": "2024-04-23T10:04:26.278141Z",
          "iopub.status.idle": "2024-04-23T10:04:40.639044Z",
          "shell.execute_reply.started": "2024-04-23T10:04:26.278117Z",
          "shell.execute_reply": "2024-04-23T10:04:40.637707Z"
        },
        "trusted": true,
        "id": "mWGxg3CO9HAE",
        "outputId": "40a6ed4b-5f95-4806-81ad-4a128860e659"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\ntensorflow 2.15.0 requires tensorboard<2.16,>=2.15, but you have tensorboard 2.16.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \"transformers==4.36.2\" \"datasets==2.16.1\" \"accelerate==0.26.1\" \"bitsandbytes==0.42.0\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T10:04:40.641505Z",
          "iopub.execute_input": "2024-04-23T10:04:40.641913Z",
          "iopub.status.idle": "2024-04-23T10:05:09.614809Z",
          "shell.execute_reply.started": "2024-04-23T10:04:40.641873Z",
          "shell.execute_reply": "2024-04-23T10:05:09.613594Z"
        },
        "trusted": true,
        "id": "RxRyflaH9HAF",
        "outputId": "4b48914b-92a2-4488-8245-9e2a939fc572"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ngcsfs 2024.2.0 requires fsspec==2024.2.0, but you have fsspec 2023.10.0 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ns3fs 2024.2.0 requires fsspec==2024.2.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install evaluate"
      ],
      "metadata": {
        "trusted": true,
        "id": "lcVE2GWL9HAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code imports the os module and sets two environment variables:\n",
        "* CUDA_VISIBLE_DEVICES: This environment variable tells PyTorch which GPUs to use. In this case, the code is setting the environment variable to 0, which means that PyTorch will use the first GPU.\n",
        "* TOKENIZERS_PARALLELISM: This environment variable tells the Hugging Face Transformers library whether to parallelize the tokenization process. In this case, the code is setting the environment variable to false, which means that the tokenization process will not be parallelized."
      ],
      "metadata": {
        "id": "NOabiSLf9HAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e\n",
        "!pip install -q -U git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T10:05:09.616335Z",
          "iopub.execute_input": "2024-04-23T10:05:09.616668Z",
          "iopub.status.idle": "2024-04-23T10:06:08.303942Z",
          "shell.execute_reply.started": "2024-04-23T10:05:09.616616Z",
          "shell.execute_reply": "2024-04-23T10:06:08.302478Z"
        },
        "trusted": true,
        "id": "_sK2-wy89HAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T10:06:55.810929Z",
          "iopub.execute_input": "2024-04-23T10:06:55.811552Z",
          "iopub.status.idle": "2024-04-23T10:06:55.816791Z",
          "shell.execute_reply.started": "2024-04-23T10:06:55.811521Z",
          "shell.execute_reply": "2024-04-23T10:06:55.815584Z"
        },
        "trusted": true,
        "id": "79rLVLqN9HAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code import warnings; warnings.filterwarnings(\"ignore\") imports the warnings module and sets the warning filter to ignore. This means that all warnings will be suppressed and will not be displayed. Actually during training there are many warnings that do not prevent the fine-tuning but can be distracting and make you wonder if you are doing the correct things."
      ],
      "metadata": {
        "id": "llWtm7U_9HAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T10:07:04.085479Z",
          "iopub.execute_input": "2024-04-23T10:07:04.085845Z",
          "iopub.status.idle": "2024-04-23T10:07:04.090412Z",
          "shell.execute_reply.started": "2024-04-23T10:07:04.085815Z",
          "shell.execute_reply": "2024-04-23T10:07:04.089526Z"
        },
        "trusted": true,
        "id": "_q3tteUU9HAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell there are all the other imports for running the notebook"
      ],
      "metadata": {
        "id": "TdofAUXC9HAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T10:07:12.10783Z",
          "iopub.execute_input": "2024-04-23T10:07:12.108441Z",
          "iopub.status.idle": "2024-04-23T10:07:12.378891Z",
          "shell.execute_reply.started": "2024-04-23T10:07:12.108411Z",
          "shell.execute_reply": "2024-04-23T10:07:12.378053Z"
        },
        "trusted": true,
        "id": "jTqzb3g29HAK",
        "outputId": "9c8cb54c-d5fb-430a-9783-29330cd6d1e6",
        "colab": {
          "referenced_widgets": [
            "bab8530d57014afeb9a47518abc5b5c3"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bab8530d57014afeb9a47518abc5b5c3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import bitsandbytes as bnb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from peft import LoraConfig, PeftConfig\n",
        "from trl import SFTTrainer\n",
        "from trl import setup_chat_format\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                          TrainingArguments,\n",
        "                          pipeline,\n",
        "                          logging)\n",
        "from sklearn.metrics import (accuracy_score,\n",
        "                             classification_report,\n",
        "                             confusion_matrix)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 14.485002,
          "end_time": "2023-10-16T11:00:18.917449",
          "exception": false,
          "start_time": "2023-10-16T11:00:04.432447",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-04-23T10:07:21.748456Z",
          "iopub.execute_input": "2024-04-23T10:07:21.749081Z",
          "iopub.status.idle": "2024-04-23T10:07:40.900732Z",
          "shell.execute_reply.started": "2024-04-23T10:07:21.74905Z",
          "shell.execute_reply": "2024-04-23T10:07:40.899927Z"
        },
        "trusted": true,
        "id": "xdi5GBmj9HAL",
        "outputId": "c8f7b312-4bda-4cf9-b326-024314e3ac1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-04-23 10:07:30.118449: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-23 10:07:30.118560: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-23 10:07:30.256193: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"pytorch version {torch.__version__}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T10:07:40.902549Z",
          "iopub.execute_input": "2024-04-23T10:07:40.902928Z",
          "iopub.status.idle": "2024-04-23T10:07:40.908448Z",
          "shell.execute_reply.started": "2024-04-23T10:07:40.902896Z",
          "shell.execute_reply": "2024-04-23T10:07:40.907438Z"
        },
        "trusted": true,
        "id": "25ZeKxLq9HAL",
        "outputId": "9cdcc793-c99d-4fb7-e481-817f835b457d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "pytorch version 2.1.2\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"working on {device}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T10:07:40.909772Z",
          "iopub.execute_input": "2024-04-23T10:07:40.910119Z",
          "iopub.status.idle": "2024-04-23T10:07:41.001666Z",
          "shell.execute_reply.started": "2024-04-23T10:07:40.910087Z",
          "shell.execute_reply": "2024-04-23T10:07:41.000702Z"
        },
        "trusted": true,
        "id": "SPRTfCi49HAM",
        "outputId": "163033ef-2597-4b0d-8ea3-794cdb7bd4ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "working on cuda:0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the data and the core evaluation functions"
      ],
      "metadata": {
        "id": "2xb_aWfM9HAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in the next cell performs the following steps:\n",
        "\n",
        "1. Reads the input dataset from the all-data.csv file, which is a comma-separated value (CSV) file with two columns: sentiment and text.\n",
        "2. Splits the dataset into training and test sets, with 300 samples in each set. The split is stratified by sentiment, so that each set contains a representative sample of positive, neutral, and negative sentiments.\n",
        "3. Shuffles the train data in a replicable order (random_state=10)\n",
        "4. Transforms the texts contained in the train and test data into prompts to be used by Llama: the train prompts contains the expected answer we want to fine-tune the model with\n",
        "5. The residual examples not in train or test, for reporting purposes during training (but it won't be used for early stopping), is treated as evaluation data, which is sampled with repetition in order to have a 50/50/50 sample (negative instances are very few, hence they should be repeated)\n",
        "5. The train and eval data are wrapped by the class from Hugging Face (https://huggingface.co/docs/datasets/index)\n",
        "\n",
        "This prepares in a single cell train_data, eval_data and test_data datasets to be used in our fine tuning."
      ],
      "metadata": {
        "id": "QYUrFx579HAO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "uo0W6YyI9HAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "\n",
        "# Step 1: Read the dataset into a DataFrame\n",
        "filename = \"/kaggle/working/merge_IEEE_annotated_Oﬀensive_tweets.csv\"\n",
        "df = pd.read_csv(filename, encoding=\"utf-8\", encoding_errors=\"replace\")\n",
        "\n",
        "# Step 2: Clean the text data\n",
        "def clean_text(text):\n",
        "    # Replace emojis with a placeholder to preserve their presence\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "\n",
        "    # Retain non-ASCII characters except for emojis\n",
        "    text = re.sub(r'[^\\x00-\\x7F]', ' ', text)\n",
        "\n",
        "    # Replace URLs with a placeholder to preserve their presence\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"<URL>\", text)\n",
        "\n",
        "    # Replace special characters and punctuation marks with a space\n",
        "    text = re.sub(r'[\\[\\]\\\"#()\\-,./:;<=>\\\\[\\]^_`!]', ' ', text)\n",
        "\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.strip()  # Strip any leading or trailing whitespace\n",
        "\n",
        "# Apply cleaning to the 'text' column\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "print(df.head())\n",
        "\n",
        "# Step 3: Remove empty rows\n",
        "df = df.dropna(subset=['text'])\n",
        "\n",
        "# Step 4: Remove duplicate rows\n",
        "df = df.drop_duplicates(subset=['text'])\n",
        "\n",
        "# Step 5: Rename columns\n",
        "df = df.rename(columns={'label': 'label'})\n",
        "\n",
        "# Specify the file path for saving the cleaned dataset\n",
        "output_filename = \"/kaggle/working/cleaned_dataset_v2_IEEE_merged_data.csv\"\n",
        "\n",
        "# Save the cleaned DataFrame to a CSV file with column headers\n",
        "df.to_csv(output_filename, index=False, header=True)\n",
        "\n",
        "print(f\"Cleaned dataset saved to {output_filename}\")\n",
        "\n",
        "label_counts = df['label'].value_counts()\n",
        "# Print the counts\n",
        "print(\"label counts:\")\n",
        "print(label_counts)\n",
        "\n",
        "\n",
        "# import pandas as pd\n",
        "# import re\n",
        "# import numpy as np\n",
        "# from datasets import Dataset\n",
        "\n",
        "\n",
        "# # Step 1: Read the dataset into a DataFrame\n",
        "# filename = \"/kaggle/working/merge_IEEE_annotated_Oﬀensive_tweets.csv\"\n",
        "# df = pd.read_csv(filename, encoding=\"utf-8\", encoding_errors=\"replace\")\n",
        "\n",
        "# # Step 2: Clean the text data\n",
        "\n",
        "# def clean_text(text):\n",
        "#     # Remove URLs\n",
        "#     text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "\n",
        "#     # Remove non-ASCII characters (including emojis)\n",
        "#     text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "\n",
        "#     # Remove punctuation except for apostrophes\n",
        "#     text = re.sub(r'[^\\w\\s\\']', '', text)\n",
        "\n",
        "#     # Replace multiple spaces with a single space\n",
        "#     text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "#     return text.strip()  # Strip any leading or trailing whitespace\n",
        "\n",
        "\n",
        "# # Apply cleaning to the 'text' column\n",
        "# df['text'] = df['text'].apply(clean_text)\n",
        "# print(df.head())\n",
        "\n",
        "# # Step 3: Remove empty rows\n",
        "# df = df.dropna(subset=['text'])\n",
        "\n",
        "# # Step 4: Remove duplicate rows\n",
        "# df = df.drop_duplicates(subset=['text'])\n",
        "\n",
        "# # Step 5: Rename columns\n",
        "# df = df.rename(columns={'label': 'label'})\n",
        "\n",
        "# # Filter out rows with label \"unknown\"\n",
        "# df = df[df['label'] != 'unknown']\n",
        "\n",
        "# ## without label (Unknown)\n",
        "# # Specify the file path for saving the cleaned dataset\n",
        "# output_filename = \"/kaggle/working/cleaned_dataset_v2_IEEE_merged_data.csv\"\n",
        "\n",
        "# # Save the cleaned DataFrame to a CSV file without including the column names in the first row\n",
        "# df.to_csv(output_filename, index=False, header=False)\n",
        "\n",
        "# print(f\"Cleaned dataset saved to {output_filename}\")\n",
        "\n",
        "# print(df.head())\n",
        "\n",
        "# label_counts = df['label'].value_counts()\n",
        "# # Print the counts\n",
        "# print(\"label counts:\")\n",
        "# print(label_counts)\n",
        "\n",
        "## with label (Unknown)\n",
        "# # Specify the file path for saving the cleaned dataset\n",
        "# output_filename = \"/kaggle/working/cleaned_dataset_v2_IEEE_merged_data.csv\"\n",
        "\n",
        "# # Save the cleaned DataFrame to a CSV file without including the column names in the first row\n",
        "# df.to_csv(output_filename, index=False, header=False)\n",
        "\n",
        "# print(f\"Cleaned dataset saved to {output_filename}\")\n",
        "\n",
        "# print(df.head())\n",
        "\n",
        "# label_counts = df['label'].value_counts()\n",
        "# # Print the counts\n",
        "# print(\"label counts:\")\n",
        "# print(label_counts)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T10:07:41.003764Z",
          "iopub.execute_input": "2024-04-23T10:07:41.004097Z",
          "iopub.status.idle": "2024-04-23T10:07:46.94978Z",
          "shell.execute_reply.started": "2024-04-23T10:07:41.004073Z",
          "shell.execute_reply": "2024-04-23T10:07:46.948808Z"
        },
        "trusted": true,
        "id": "uN8yxttx9HAP",
        "outputId": "0cc8b88d-4337-4277-e714-a7de0aaf0652"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "                                                text       label\n0  Data leak from Huazhu Hotels may affect 130 mi...      threat\n1  good slides The Advanced Exploitation of 64 bi...      threat\n2  CVE 2018 1000532 beep URL beep version 1 3 and...      threat\n3  Will upload some of yesterday's videos which d...      threat\n4  You can t get to courage without walking throu...  irrelevant\nCleaned dataset saved to /kaggle/working/cleaned_dataset_v2_IEEE_merged_data.csv\nlabel counts:\nlabel\nthreat        6159\nirrelevant    5824\nunknown       3284\nName: count, dtype: int64\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B2zuP7sR9HAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "X_train = list()\n",
        "X_test = list()\n",
        "\n",
        "for label in [\"irrelevant\", \"threat\", \"unknown\"]:\n",
        "    train, test  = train_test_split(df[df.label==label],\n",
        "                                    train_size=0.85,\n",
        "                                    test_size=0.10,\n",
        "                                    random_state=42)\n",
        "    X_train.append(train)\n",
        "    X_test.append(test)\n",
        "\n",
        "X_train = pd.concat(X_train).sample(frac=1, random_state=10)\n",
        "X_test = pd.concat(X_test)\n",
        "\n",
        "eval_idx = [idx for idx in df.index if idx not in list(X_train.index) + list(X_test.index)]\n",
        "X_eval = df[df.index.isin(eval_idx)]\n",
        "X_eval = (X_eval\n",
        "          .groupby('label', group_keys=False)\n",
        "          .apply(lambda x: x.sample(n=100, random_state=10, replace=True)))\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "\n",
        "# Count occurrences of labels in each dataset\n",
        "train_label_counts = X_train['label'].value_counts()\n",
        "test_label_counts = X_test['label'].value_counts()\n",
        "eval_label_counts = X_eval['label'].value_counts()\n",
        "\n",
        "# Print the counts\n",
        "print(\"Train label counts:\")\n",
        "print(train_label_counts)\n",
        "\n",
        "print(\"\\nTest label counts:\")\n",
        "print(test_label_counts)\n",
        "\n",
        "print(\"\\nEvaluation label counts:\")\n",
        "print(eval_label_counts)\n",
        "\n",
        "\n",
        "# Define prompt generation functions\n",
        "def generate_prompt(data_point):\n",
        "    return f\"\"\"\n",
        "            You are an intelligent chatbot. Analyze the following text and return the answer as the corresponding label \"irrelevant\" or \"threat\" or \"unknown\".\n",
        "\n",
        "            [{data_point[\"text\"]}] = {data_point[\"label\"]}\n",
        "            \"\"\".strip()\n",
        "\n",
        "def generate_test_prompt(data_point):\n",
        "    return f\"\"\"\n",
        "            You are an intelligent chatbot. Analyze the following text and return the answer as the corresponding label \"irrelevant\" or \"threat\" or \"unknown\".\n",
        "\n",
        "            [{data_point[\"text\"]}] = \"\"\".strip()\n",
        "\n",
        "X_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1),\n",
        "                       columns=[\"text\"])\n",
        "X_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1),\n",
        "                      columns=[\"text\"])\n",
        "\n",
        "y_true = X_test.label\n",
        "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n",
        "\n",
        "train_data = Dataset.from_pandas(X_train)\n",
        "eval_data = Dataset.from_pandas(X_eval)\n",
        "\n",
        "# Print dataset shapes and preview\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of X_eval:\", X_eval.shape)\n",
        "\n",
        "print(\"X_train:\")\n",
        "print(X_train.head())\n",
        "\n",
        "print(\"X_test:\")\n",
        "print(X_test.head())\n",
        "\n",
        "print(\"X_eval:\")\n",
        "print(X_eval.head())\n",
        "\n",
        "print(\"===============================================================\")\n",
        "# Assuming you want to print text from the first cell of X_train\n",
        "print(X_train.iloc[0][0])\n",
        "print(\"===============================================================\")\n",
        "\n",
        "print(X_test.iloc[0][0])\n",
        "print(\"===============================================================\")\n",
        "\n",
        "print(X_eval.iloc[0][0])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T10:07:46.951496Z",
          "iopub.execute_input": "2024-04-23T10:07:46.952088Z",
          "iopub.status.idle": "2024-04-23T10:08:18.14474Z",
          "shell.execute_reply.started": "2024-04-23T10:07:46.95205Z",
          "shell.execute_reply": "2024-04-23T10:08:18.143677Z"
        },
        "trusted": true,
        "id": "6KO00uct9HAT",
        "outputId": "3675d9c4-b4be-4293-f397-09e98da53f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Train label counts:\nlabel\nthreat        5235\nirrelevant    4950\nunknown       2791\nName: count, dtype: int64\n\nTest label counts:\nlabel\nthreat        616\nirrelevant    583\nunknown       329\nName: count, dtype: int64\n\nEvaluation label counts:\nlabel\nirrelevant    100\nthreat        100\nunknown       100\nName: count, dtype: int64\nShape of X_train: (12976, 1)\nShape of X_test: (1528, 1)\nShape of X_eval: (300, 1)\nX_train:\n                                                text\n0  You are an intelligent chatbot. Analyze the fo...\n1  You are an intelligent chatbot. Analyze the fo...\n2  You are an intelligent chatbot. Analyze the fo...\n3  You are an intelligent chatbot. Analyze the fo...\n4  You are an intelligent chatbot. Analyze the fo...\nX_test:\n                                                    text\n14389  You are an intelligent chatbot. Analyze the fo...\n7223   You are an intelligent chatbot. Analyze the fo...\n5047   You are an intelligent chatbot. Analyze the fo...\n5707   You are an intelligent chatbot. Analyze the fo...\n17107  You are an intelligent chatbot. Analyze the fo...\nX_eval:\n                                                    text\n17129  You are an intelligent chatbot. Analyze the fo...\n10515  You are an intelligent chatbot. Analyze the fo...\n1125   You are an intelligent chatbot. Analyze the fo...\n10446  You are an intelligent chatbot. Analyze the fo...\n11852  You are an intelligent chatbot. Analyze the fo...\n===============================================================\nYou are an intelligent chatbot. Analyze the following text and return the answer as the corresponding label \"irrelevant\" or \"threat\" or \"unknown\".\n\n            [Huawei Denial of Service Vulnerability URL] = threat\n===============================================================\nYou are an intelligent chatbot. Analyze the following text and return the answer as the corresponding label \"irrelevant\" or \"threat\" or \"unknown\".\n\n            [@ all time l0w Glad to hear you're feeling a bit better the whole vulnerability thing is a head scratcher sometimes hard to move from the bed or the couch sometimes I'm glad today is Sunday pressure is off a little smiling face with smiling eyes green heart victory hand] =\n===============================================================\nYou are an intelligent chatbot. Analyze the following text and return the answer as the corresponding label \"irrelevant\" or \"threat\" or \"unknown\".\n\n            [@smmsharp I AM DYING rolling on the floor laughing] = irrelevant\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Z6kiGfCO9HA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(y_true, y_pred):\n",
        "    labels = [\"unknown\", \"irrelevant\", \"threat\"]\n",
        "    mapping = {\"unknown\": 0, \"irrelevant\": 1, \"threat\": 2}\n",
        "    def map_func(x):\n",
        "        return mapping.get(x, 0)\n",
        "\n",
        "    y_true = np.vectorize(map_func)(y_true)\n",
        "    y_pred = np.vectorize(map_func)(y_pred)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
        "    print(f'Accuracy: {accuracy:.3f}')\n",
        "\n",
        "    # Generate accuracy report\n",
        "    unique_labels = set(y_true)  # Get unique labels\n",
        "\n",
        "    for label in unique_labels:\n",
        "        label_indices = [i for i in range(len(y_true))\n",
        "                         if y_true[i] == label]\n",
        "        label_y_true = [y_true[i] for i in label_indices]\n",
        "        label_y_pred = [y_pred[i] for i in label_indices]\n",
        "        accuracy = accuracy_score(label_y_true, label_y_pred)\n",
        "        print(f'Accuracy for label {label}: {accuracy:.3f}')\n",
        "\n",
        "    # Generate classification report\n",
        "    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n",
        "    print('\\nClassification Report:')\n",
        "    print(class_report)\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n",
        "    print('\\nConfusion Matrix:')\n",
        "    print(conf_matrix)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T10:09:12.2465Z",
          "iopub.execute_input": "2024-04-23T10:09:12.247341Z",
          "iopub.status.idle": "2024-04-23T10:09:12.256546Z",
          "shell.execute_reply.started": "2024-04-23T10:09:12.247308Z",
          "shell.execute_reply": "2024-04-23T10:09:12.255572Z"
        },
        "trusted": true,
        "id": "A9nVXV1K9HA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from evaluate import load\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# f1_score_2 = load(\"f1\")\n",
        "\n",
        "# def compute_metrics(eval_pred):\n",
        "#     predictions, labels = eval_pred\n",
        "#     predictions = np.argmax(predictions, axis=1)\n",
        "#     return f1_score_2.compute(predictions=predictions, references=labels, average='weighted')\n",
        "\n",
        "# Example usage:\n",
        "# eval_pred = (predictions, labels)\n",
        "# metrics = compute_metrics(eval_pred)\n",
        "# print(metrics)\n",
        "# Setup evaluation\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "trusted": true,
        "id": "VjRL3JF69HA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "UWQ4nnXT9HA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we create a function to evaluate the results from our fine-tuned sentiment model. The function performs the following steps:\n",
        "\n",
        "1. Maps the sentiment labels to a numerical representation, where 2 represents positive, 1 represents neutral, and 0 represents negative.\n",
        "2. Calculates the accuracy of the model on the test data.\n",
        "3. Generates an accuracy report for each sentiment label.\n",
        "4. Generates a classification report for the model.\n",
        "5. Generates a confusion matrix for the model."
      ],
      "metadata": {
        "id": "6L0kuuix9HA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the model without fine-tuning"
      ],
      "metadata": {
        "id": "MfzyRdBi9HA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to take care of the model, which is a 7b-hf (7 billion parameters, no RLHF, in the HuggingFace compatible format), loading from Kaggle models and quantization.\n",
        "\n",
        "Model loading and quantization:\n",
        "\n",
        "* First the code loads the Llama-2 language model from the Hugging Face Hub.\n",
        "* Then the code gets the float16 data type from the torch library. This is the data type that will be used for the computations.\n",
        "* Next, it creates a BitsAndBytesConfig object with the following settings:\n",
        "    1. load_in_4bit: Load the model weights in 4-bit format.\n",
        "    2. bnb_4bit_quant_type: Use the \"nf4\" quantization type. 4-bit NormalFloat (NF4), is a new data type that is information theoretically optimal for normally distributed weights.\n",
        "    3. bnb_4bit_compute_dtype: Use the float16 data type for computations.\n",
        "    4. bnb_4bit_use_double_quant: Do not use double quantization (reduces the average memory footprint by quantizing also the quantization constants and saves an additional 0.4 bits per parameter.).\n",
        "* Then the code creates a AutoModelForCausalLM object from the pre-trained Llama-2 language model, using the BitsAndBytesConfig object for quantization.\n",
        "* After that, the code disables caching for the model.\n",
        "* Finally the code sets the pre-training token probability to 1.\n",
        "\n",
        "Tokenizer loading:\n",
        "\n",
        "* First, the code loads the tokenizer for the Llama-2 language model.\n",
        "* Then it sets the padding token to be the end-of-sequence (EOS) token.\n",
        "* Finally, the code sets the padding side to be \"right\", which means that the input sequences will be padded on the right side. This is crucial for correct padding direction (this is the way with Llama 2)."
      ],
      "metadata": {
        "id": "Ak0QX1oN9HBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for empty the output path\n",
        "import shutil\n",
        "shutil.rmtree(\"/kaggle/working/\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "n1Q6oUn59HBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "for _ in range(100):\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T22:53:47.198517Z",
          "iopub.execute_input": "2024-04-16T22:53:47.198931Z",
          "iopub.status.idle": "2024-04-16T22:54:27.43495Z",
          "shell.execute_reply.started": "2024-04-16T22:53:47.1989Z",
          "shell.execute_reply": "2024-04-16T22:54:27.433885Z"
        },
        "trusted": true,
        "id": "aLVzyA5v9HBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes as bnb\n",
        "\n",
        "\n",
        "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
        "\n",
        "\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=device,\n",
        "    torch_dtype=compute_dtype,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
        "                                          trust_remote_code=True,\n",
        "                                         )\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T10:11:25.041932Z",
          "iopub.execute_input": "2024-04-23T10:11:25.042289Z",
          "iopub.status.idle": "2024-04-23T10:13:21.726056Z",
          "shell.execute_reply.started": "2024-04-23T10:11:25.042262Z",
          "shell.execute_reply": "2024-04-23T10:13:21.725219Z"
        },
        "trusted": true,
        "id": "OA6T0CrX9HBC",
        "outputId": "22bb63d3-037c-4817-b98b-d5265249950b",
        "colab": {
          "referenced_widgets": [
            "7febd7dcf01e4a1d94a186896c0d34bc"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7febd7dcf01e4a1d94a186896c0d34bc"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell, we set a function for predicting the sentiment of a news headline using the Llama-2 language model. The function takes three arguments:\n",
        "\n",
        "test: A Pandas DataFrame containing the news headlines to be predicted.\n",
        "model: The pre-trained Llama-2 language model.\n",
        "tokenizer: The tokenizer for the Llama-2 language model.\n",
        "\n",
        "The function works as follows:\n",
        "\n",
        "1. For each news headline in the test DataFrame:\n",
        "    * Create a prompt for the language model, which asks it to analyze the sentiment of the news headline and return the corresponding sentiment label.\n",
        "    * Use the pipeline() function from the Hugging Face Transformers library to generate text from the language model, using the prompt.\n",
        "    * Extract the predicted sentiment label from the generated text.\n",
        "    * Append the predicted sentiment label to the y_pred list.\n",
        "2. Return the y_pred list.\n",
        "\n",
        "The pipeline() function from the Hugging Face Transformers library is used to generate text from the language model. The task argument specifies that the task is text generation. The model and tokenizer arguments specify the pre-trained Llama-2 language model and the tokenizer for the language model. The max_new_tokens argument specifies the maximum number of new tokens to generate. The temperature argument controls the randomness of the generated text. A lower temperature will produce more predictable text, while a higher temperature will produce more creative and unexpected text.\n",
        "\n",
        "The if statement checks if the generated text contains the word \"positive\". If it does, then the predicted sentiment label is \"positive\". Otherwise, the if statement checks if the generated text contains the word \"negative\". If it does, then the predicted sentiment label is \"negative\". Otherwise, the if statement checks if the generated text contains the word \"neutral\". If it does, then the predicted sentiment label is \"neutral."
      ],
      "metadata": {
        "id": "5uJfpcE_9HBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm import tqdm\n",
        "# def predict(X_test, model, tokenizer):\n",
        "#     y_pred = []\n",
        "#     for i in tqdm(range(len(X_test))):\n",
        "#         prompt = X_test.iloc[i][\"text\"]\n",
        "#         pipe = pipeline(task=\"text-generation\",\n",
        "#                         model=model,\n",
        "#                         tokenizer=tokenizer,\n",
        "#                         max_new_tokens = 1,\n",
        "#                         do_sample=False\n",
        "#                        )\n",
        "#         result = pipe(prompt)\n",
        "#         answer = result[0]['generated_text'].split(\"=\")[-1]\n",
        "#         if \"threat\" in answer:\n",
        "#             y_pred.append(\"threat\")\n",
        "#         else:\n",
        "#             y_pred.append(\"normal\")\n",
        "#     return y_pred\n",
        "\n",
        "def predict(test: pd.DataFrame, model: AutoModelForCausalLM, tokenizer: AutoTokenizer) -> list:\n",
        "    y_pred = []\n",
        "    pipe = pipeline(task=\"text-generation\",\n",
        "                        model=model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        max_new_tokens = 5,\n",
        "                        do_sample=False\n",
        "                       )\n",
        "\n",
        "    for i in tqdm(range(len(test))):\n",
        "        prompt = test.iloc[i][\"text\"]\n",
        "        result = pipe(prompt)\n",
        "\n",
        "        answer = result[0]['generated_text'].split(\"=\")[-1]\n",
        "\n",
        "        if \"threat\" in answer:\n",
        "            y_pred.append(\"threat\")\n",
        "        elif \"irrelevant\" in answer:\n",
        "            y_pred.append(\"irrelevant\")\n",
        "        elif \"unknown\" in answer:\n",
        "            y_pred.append(\"unknown\")\n",
        "        else:\n",
        "            y_pred.append(\"none\")\n",
        "    return y_pred\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T10:13:21.727708Z",
          "iopub.execute_input": "2024-04-23T10:13:21.728021Z",
          "iopub.status.idle": "2024-04-23T10:13:21.737492Z",
          "shell.execute_reply.started": "2024-04-23T10:13:21.727995Z",
          "shell.execute_reply": "2024-04-23T10:13:21.73666Z"
        },
        "trusted": true,
        "id": "IpJq3yjY9HBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we are ready to test the Llama 2 7b-hf model and see how it performs on our problem without any fine-tuning. This allows us to get insights on the model itself and establish a baseline."
      ],
      "metadata": {
        "id": "-90fGPdY9HBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = predict(X_test, model, tokenizer)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T10:13:21.738532Z",
          "iopub.execute_input": "2024-04-23T10:13:21.738868Z",
          "iopub.status.idle": "2024-04-23T10:29:13.816861Z",
          "shell.execute_reply.started": "2024-04-23T10:13:21.738843Z",
          "shell.execute_reply": "2024-04-23T10:29:13.815949Z"
        },
        "trusted": true,
        "id": "vOPanJAs9HBM",
        "outputId": "b77fa9c2-9d6e-4382-dd81-26c581730a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|██████████| 1528/1528 [15:52<00:00,  1.60it/s]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell, we evaluate the results. There is little to be said, it is performing really terribly because the 7b-hf model tends to just predict a neutral sentiment and seldom it detects positive or negative sentiment."
      ],
      "metadata": {
        "id": "3NUPhDzb9HBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation(y_true, y_pred)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T10:29:13.818552Z",
          "iopub.execute_input": "2024-04-23T10:29:13.81892Z",
          "iopub.status.idle": "2024-04-23T10:29:13.84216Z",
          "shell.execute_reply.started": "2024-04-23T10:29:13.818893Z",
          "shell.execute_reply": "2024-04-23T10:29:13.841134Z"
        },
        "trusted": true,
        "id": "PHr6NDbr9HBN",
        "outputId": "f91e6cca-152f-498b-c2be-e6ee5d4e24f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 0.392\nAccuracy for label 0: 0.286\nAccuracy for label 1: 0.846\nAccuracy for label 2: 0.019\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.23      0.29      0.25       329\n           1       0.45      0.85      0.59       583\n           2       0.44      0.02      0.04       616\n\n    accuracy                           0.39      1528\n   macro avg       0.37      0.38      0.29      1528\nweighted avg       0.40      0.39      0.29      1528\n\n\nConfusion Matrix:\n[[ 94 226   9]\n [ 84 493   6]\n [239 365  12]]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "evaluation = pd.DataFrame({'text': X_test[\"text\"],\n",
        "                           'y_true':y_true,\n",
        "                           'y_pred': y_pred},\n",
        "                         )\n",
        "evaluation.to_csv(\"test_before_fine-tune_predictions.csv\", index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T03:38:18.645288Z",
          "iopub.execute_input": "2024-04-20T03:38:18.645994Z",
          "iopub.status.idle": "2024-04-20T03:38:18.675486Z",
          "shell.execute_reply.started": "2024-04-20T03:38:18.645949Z",
          "shell.execute_reply": "2024-04-20T03:38:18.674391Z"
        },
        "trusted": true,
        "id": "M84KEeLN9HBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def generate_test_prompt2(data_point):\n",
        "#     return f\"\"\"\n",
        "#             You are an intelligent chatbot. Analyze the following text and return the answer as the corresponding label \"irrelevant\" or one of the following cybercrime types: \"threat: 'vulnerability'\", \"threat: 'general'\", \"threat: 'ransomware'\", \"threat: 'ddos'\", \"threat: 'Offensive_Language'\", \"threat: 'botnet'\".\n",
        "\n",
        "#             [{data_point[\"text\"]}]\n",
        "#             \"\"\".strip()\n",
        "\n",
        "\n",
        "# # Define some new text data points\n",
        "# new_data = [\n",
        "#     \"I just received an email asking for my bank account information. This seems suspicious.\",\n",
        "#     \"Here's a link to a great resource for learning about cyber security!\",\n",
        "#     \"I'm being threatened online! What should I do?\",\n",
        "# ]\n",
        "\n",
        "# # Preprocess the new data (similar to how you processed X_test)\n",
        "# new_data_df = pd.DataFrame({\"text\": new_data})\n",
        "# new_data_prompts = new_data_df.apply(generate_test_prompt2, axis=1)  # Replace with your prompt generation function\n",
        "# new_data_df = pd.DataFrame(new_data_prompts, columns=[\"text\"])\n",
        "\n",
        "# # Make predictions on the new data\n",
        "# new_predictions = predict(new_data_df, model, tokenizer)\n",
        "\n",
        "# # Print the predictions\n",
        "# for i, text in enumerate(new_data):\n",
        "#   print(f\"Text: {text}\")\n",
        "#   print(f\"Predicted Label: {new_predictions[i]}\")\n",
        "#   print(\"-------\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T04:47:48.396754Z",
          "iopub.execute_input": "2024-04-15T04:47:48.397463Z",
          "iopub.status.idle": "2024-04-15T04:47:50.44525Z",
          "shell.execute_reply.started": "2024-04-15T04:47:48.39743Z",
          "shell.execute_reply": "2024-04-15T04:47:50.444298Z"
        },
        "trusted": true,
        "id": "LIVc_Wua9HBO",
        "outputId": "818f059b-8230-461c-8852-172cebf39fc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|██████████| 3/3 [00:02<00:00,  1.47it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Text: I just received an email asking for my bank account information. This seems suspicious.\nPredicted Label: threat: 'vulnerability'\n-------\nText: Here's a link to a great resource for learning about cyber security!\nPredicted Label: threat: 'vulnerability'\n-------\nText: I'm being threatened online! What should I do?\nPredicted Label: threat: 'vulnerability'\n-------\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning"
      ],
      "metadata": {
        "id": "LNNCCSPk9HBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell we set everything ready for the fine-tuning. We configures and initializes a Simple Fine-tuning Trainer (SFTTrainer) for training a large language model using the Parameter-Efficient Fine-Tuning (PEFT) method, which should save time as it operates on a reduced number of parameters compared to the model's overall size. The PEFT method focuses on refining a limited set of (additional) model parameters, while keeping the majority of the pre-trained LLM parameters fixed. This significantly reduces both computational and storage expenses. Additionally, this strategy addresses the challenge of catastrophic forgetting, which often occurs during the complete fine-tuning of LLMs.\n",
        "\n",
        "PEFTConfig:\n",
        "\n",
        "The peft_config object specifies the parameters for PEFT. The following are some of the most important parameters:\n",
        "\n",
        "* lora_alpha: The learning rate for the LoRA update matrices.\n",
        "* lora_dropout: The dropout probability for the LoRA update matrices.\n",
        "* r: The rank of the LoRA update matrices.\n",
        "* bias: The type of bias to use. The possible values are none, additive, and learned.\n",
        "* task_type: The type of task that the model is being trained for. The possible values are CAUSAL_LM and MASKED_LM.\n",
        "\n",
        "TrainingArguments:\n",
        "\n",
        "The training_arguments object specifies the parameters for training the model. The following are some of the most important parameters:\n",
        "\n",
        "* output_dir: The directory where the training logs and checkpoints will be saved.\n",
        "* num_train_epochs: The number of epochs to train the model for.\n",
        "* per_device_train_batch_size: The number of samples in each batch on each device.\n",
        "* gradient_accumulation_steps: The number of batches to accumulate gradients before updating the model parameters.\n",
        "* optim: The optimizer to use for training the model.\n",
        "* save_steps: The number of steps after which to save a checkpoint.\n",
        "* logging_steps: The number of steps after which to log the training metrics.\n",
        "* learning_rate: The learning rate for the optimizer.\n",
        "* weight_decay: The weight decay parameter for the optimizer.\n",
        "* fp16: Whether to use 16-bit floating-point precision.\n",
        "* bf16: Whether to use BFloat16 precision.\n",
        "* max_grad_norm: The maximum gradient norm.\n",
        "* max_steps: The maximum number of steps to train the model for.\n",
        "* warmup_ratio: The proportion of the training steps to use for warming up the learning rate.\n",
        "* group_by_length: Whether to group the training samples by length.\n",
        "* lr_scheduler_type: The type of learning rate scheduler to use.\n",
        "* report_to: The tools to report the training metrics to.\n",
        "* evaluation_strategy: The strategy for evaluating the model during training.\n",
        "\n",
        "SFTTrainer:\n",
        "\n",
        "The SFTTrainer is a custom trainer class from the TRL library. It is used to train large language models (also using the PEFT method).\n",
        "\n",
        "The SFTTrainer object is initialized with the following arguments:\n",
        "\n",
        "* model: The model to be trained.\n",
        "* train_dataset: The training dataset.\n",
        "* eval_dataset: The evaluation dataset.\n",
        "* peft_config: The PEFT configuration.\n",
        "* dataset_text_field: The name of the text field in the dataset.\n",
        "* tokenizer: The tokenizer to use.\n",
        "* args: The training arguments.\n",
        "* packing: Whether to pack the training samples.\n",
        "* max_seq_length: The maximum sequence length.\n",
        "\n",
        "Once the SFTTrainer object is initialized, it can be used to train the model by calling the train() method"
      ],
      "metadata": {
        "id": "8HcT4XnZ9HBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "for _ in range(100):\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T04:09:34.901185Z",
          "iopub.execute_input": "2024-04-20T04:09:34.901856Z",
          "iopub.status.idle": "2024-04-20T04:10:14.368411Z",
          "shell.execute_reply.started": "2024-04-20T04:09:34.901823Z",
          "shell.execute_reply": "2024-04-20T04:10:14.367202Z"
        },
        "trusted": true,
        "id": "4mJDeJUe9HBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import evaluate\n",
        "\n",
        "output_dir=\"v7_trained_weigths\"\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16, # learning rate for the LoRA update matrices\n",
        "    lora_dropout=0.05, # dropout probability for the LoRA update matrices\n",
        "    r=64,#rank of the LoRA update matrices, lower rank results in smaller update matrices with fewer trainable parameters\n",
        "    bias=\"none\", #type of bias to use. The possible values are none, additive, and learned.\n",
        "    task_type=\"CAUSAL_LM\", #type of task that the model is being trained for. The possible values are CAUSAL_LM and MASKED_LM\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"],\n",
        ")\n",
        "\n",
        "# training_arguments = TrainingArguments(\n",
        "#     output_dir=output_dir,                    # directory to save and repository id\n",
        "#     num_train_epochs=1, #The number of epochs to train the model for\n",
        "#     per_device_train_batch_size=4, #The number of samples in each batch on each device.\n",
        "#     per_device_eval_batch_size=4,\n",
        "#     gradient_accumulation_steps=4, # 4 The number of batches to accumulate gradients before updating the model parameters.\n",
        "#     optim=\"paged_adamw_32bit\", #optimizer to use for training the model\n",
        "#     save_steps=0, #number of steps after which to save a checkpoint\n",
        "#     logging_steps=25, #number of steps after which to log the training metrics\n",
        "#     learning_rate=1e-4, #learning rate for the optimizer\n",
        "#     weight_decay=0.001, #weight decay parameter for the optimizer\n",
        "#     fp16=True, #Whether to use 16-bit floating-point precision.\n",
        "#     bf16=False, #Whether to use BFloat16 precision.\n",
        "#     max_grad_norm=0.3, #The maximum gradient norm\n",
        "#     max_steps=-1, #The maximum number of steps to train the model for\n",
        "#     warmup_ratio=0.03,#The proportion of the training steps to use for warming up the learning rate.\n",
        "#     group_by_length=True, #Whether to group the training samples by length.\n",
        "#     lr_scheduler_type=\"cosine\", # type of learning rate scheduler to use\n",
        "#     report_to=\"tensorboard\", # The tools to report the training metrics to\n",
        "#     evaluation_strategy=\"epoch\" #strategy for evaluating the model during training\n",
        "# )\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,                    # directory to save and repository id\n",
        "    num_train_epochs=1, #The number of epochs to train the model for\n",
        "    per_device_train_batch_size=2, #The number of samples in each batch on each device.\n",
        "    gradient_accumulation_steps=4, # 4 The number of batches to accumulate gradients before updating the model parameters.\n",
        "    optim=\"paged_adamw_32bit\", #optimizer to use for training the model\n",
        "    save_steps=0, #number of steps after which to save a checkpoint\n",
        "    logging_steps=25, #number of steps after which to log the training metrics\n",
        "    learning_rate=2e-5, #learning rate for the optimizer\n",
        "    weight_decay=0.001, #weight decay parameter for the optimizer\n",
        "    fp16=True, #Whether to use 16-bit floating-point precision.\n",
        "    bf16=False, #Whether to use BFloat16 precision.\n",
        "    max_grad_norm=0.3, #The maximum gradient norm\n",
        "    max_steps=-1, #The maximum number of steps to train the model for\n",
        "    warmup_ratio=0.03,#The proportion of the training steps to use for warming up the learning rate.\n",
        "    group_by_length=True, #Whether to group the training samples by length.\n",
        "    lr_scheduler_type=\"cosine\", # type of learning rate scheduler to use\n",
        "    report_to=\"tensorboard\", # The tools to report the training metrics to\n",
        "    evaluation_strategy=\"epoch\" #strategy for evaluating the model during training\n",
        ")\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=eval_data,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=1024,\n",
        "    packing=False,\n",
        "#     compute_metrics=compute_metrics,\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False,\n",
        "        \"append_concat_token\": False,\n",
        "    }\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T04:18:23.280335Z",
          "iopub.execute_input": "2024-04-20T04:18:23.281004Z",
          "iopub.status.idle": "2024-04-20T04:18:27.962796Z",
          "shell.execute_reply.started": "2024-04-20T04:18:23.280974Z",
          "shell.execute_reply": "2024-04-20T04:18:27.961895Z"
        },
        "trusted": true,
        "id": "J7MLY50P9HBX",
        "outputId": "eaa81849-b6fa-4abc-e2e4-99955824b2c5",
        "colab": {
          "referenced_widgets": [
            "a2734ab4826b411290b16604ad658144",
            "019e71e76e834975801945ad43ae0aa0"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/12976 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2734ab4826b411290b16604ad658144"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/300 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "019e71e76e834975801945ad43ae0aa0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ct3N2XqY9HBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will train the model using the trainer.train() method and then save the trained model to the trained-model directory. Using The standard GPU P100 offered by Kaggle, the training should be quite fast."
      ],
      "metadata": {
        "id": "GPd2lTZ09HBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "trainer.train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T04:18:40.531701Z",
          "iopub.execute_input": "2024-04-20T04:18:40.532516Z",
          "iopub.status.idle": "2024-04-20T07:51:40.432096Z",
          "shell.execute_reply.started": "2024-04-20T04:18:40.532483Z",
          "shell.execute_reply": "2024-04-20T07:51:40.431266Z"
        },
        "trusted": true,
        "id": "CtV3lF6G9HBZ",
        "outputId": "5cb9a7f5-90c6-491f-e639-4d98d485c8c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1622' max='1622' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1622/1622 3:32:45, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.089400</td>\n      <td>1.401954</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=1622, training_loss=1.527285056108317, metrics={'train_runtime': 12777.7032, 'train_samples_per_second': 1.016, 'train_steps_per_second': 0.127, 'total_flos': 4.369081628486861e+16, 'train_loss': 1.527285056108317, 'epoch': 1.0})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cf5XPEHv9HBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model and the tokenizer are saved to disk for later usage."
      ],
      "metadata": {
        "id": "mL5Zj2D59HBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained model and tokenizer\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T07:51:40.433611Z",
          "iopub.execute_input": "2024-04-20T07:51:40.433929Z",
          "iopub.status.idle": "2024-04-20T07:51:44.027957Z",
          "shell.execute_reply.started": "2024-04-20T07:51:40.433904Z",
          "shell.execute_reply": "2024-04-20T07:51:44.026998Z"
        },
        "trusted": true,
        "id": "euHWEgHv9HBb",
        "outputId": "32b4458e-29e2-499c-9d0b-c4982d67131f"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 22,
          "output_type": "execute_result",
          "data": {
            "text/plain": "('v7_trained_weigths/tokenizer_config.json',\n 'v7_trained_weigths/special_tokens_map.json',\n 'v7_trained_weigths/tokenizer.model',\n 'v7_trained_weigths/added_tokens.json',\n 'v7_trained_weigths/tokenizer.json')"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(\"AmirlyPhd/trainer_V7_merged_all_IEEEdatasets_threat_cases\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T07:52:43.76606Z",
          "iopub.execute_input": "2024-04-20T07:52:43.766423Z",
          "iopub.status.idle": "2024-04-20T07:52:54.112198Z",
          "shell.execute_reply.started": "2024-04-20T07:52:43.76639Z",
          "shell.execute_reply": "2024-04-20T07:52:54.111237Z"
        },
        "trusted": true,
        "id": "-p-gb14S9HBd",
        "outputId": "ab787743-54a1-49b4-e1be-c7a674f254cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/plain": "CommitInfo(commit_url='https://huggingface.co/AmirlyPhd/v7_trained_weigths/commit/06799a27d90c7c1a7b6d6aef313058298dc716a4', commit_message='AmirlyPhd/trainer_V7_merged_all_IEEEdatasets_threat_cases', commit_description='', oid='06799a27d90c7c1a7b6d6aef313058298dc716a4', pr_url=None, pr_revision=None, pr_num=None)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "6cZf2Wxi9HBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afterwards, loading the TensorBoard extension and start TensorBoard, pointing to the logs/runs directory, which is assumed to contain the training logs and checkpoints for your model, will allow you to understand how the models fits during the training."
      ],
      "metadata": {
        "id": "S0pro8019HBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard_dir=\"/kaggle/working/trained_weigths/runs/Apr07_00-37-46_7e75014457a5/events.out.tfevents.1712450278.7e75014457a5.225.0\"\n",
        "\n",
        "\n",
        "# OUTPUT_DIR = \"experiments\"\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tensorboard_dir"
      ],
      "metadata": {
        "trusted": true,
        "id": "K6hdxCLe9HBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard"
      ],
      "metadata": {
        "trusted": true,
        "id": "stcWalFE9HBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving model to disk for later usage"
      ],
      "metadata": {
        "id": "WvBoxxnw9HBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, in order to demonstrate how to re-utilize the model, we reload it from the disk and merge it with the original LLama model.\n",
        "\n",
        "In fact, when working with QLoRA, we exclusively train adapters instead of the entire model. So, when you save the model during training, you're only preserving the adapter weights, not the entire model. If you want to save the full model for easier use with Text Generation Inference, you can merge the adapter weights into the model weights using the merge_and_unload method. Then, you can save the model using the save_pretrained method. This will create a default model that's ready for inference tasks."
      ],
      "metadata": {
        "id": "SOgHggHr9HBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before proceeding, we first remove the previous model and clean up the memory from various objects we won't use anymore."
      ],
      "metadata": {
        "id": "YVlW4Lbl9HBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "del [model, tokenizer, peft_config, trainer, train_data, eval_data, bnb_config, training_arguments]\n",
        "del [df, X_train, X_eval]\n",
        "del [TrainingArguments, SFTTrainer, LoraConfig, BitsAndBytesConfig]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T07:52:54.113366Z",
          "iopub.execute_input": "2024-04-20T07:52:54.113709Z",
          "iopub.status.idle": "2024-04-20T07:52:54.137433Z",
          "shell.execute_reply.started": "2024-04-20T07:52:54.11367Z",
          "shell.execute_reply": "2024-04-20T07:52:54.135966Z"
        },
        "trusted": true,
        "id": "m7Rn5TM-9HBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "for _ in range(100):\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T07:52:54.138649Z",
          "iopub.execute_input": "2024-04-20T07:52:54.138957Z",
          "iopub.status.idle": "2024-04-20T07:53:28.148234Z",
          "shell.execute_reply.started": "2024-04-20T07:52:54.138931Z",
          "shell.execute_reply": "2024-04-20T07:53:28.147445Z"
        },
        "trusted": true,
        "id": "JYV7W3Sr9HBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "trusted": true,
        "id": "JIOq8f7K9HBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can proceed to merging the weights and we will be using the merged model for our testing purposes."
      ],
      "metadata": {
        "id": "RrYcol1P9HBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "finetuned_model = \"./v7_trained_weigths/\"\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "     finetuned_model,\n",
        "     torch_dtype=compute_dtype,\n",
        "     return_dict=False,\n",
        "     low_cpu_mem_usage=True,\n",
        "     device_map=device,\n",
        ")\n",
        "\n",
        "merged_model = model.merge_and_unload()\n",
        "merged_model.save_pretrained(\"./merged_model_V7_merged_all_IEEEdatasets_threat\",safe_serialization=True, max_shard_size=\"2GB\")\n",
        "tokenizer.save_pretrained(\"./merged_model_V7_merged_all_IEEEdatasets_threat\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T07:53:28.150322Z",
          "iopub.execute_input": "2024-04-20T07:53:28.150692Z",
          "iopub.status.idle": "2024-04-20T07:54:37.866829Z",
          "shell.execute_reply.started": "2024-04-20T07:53:28.150662Z",
          "shell.execute_reply": "2024-04-20T07:54:37.865681Z"
        },
        "trusted": true,
        "id": "9Gc7v7mT9HBm",
        "outputId": "31ec47be-63cf-446d-8974-53402901abb7",
        "colab": {
          "referenced_widgets": [
            "7b5702adf3204397940453351c9c7070"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b5702adf3204397940453351c9c7070"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
          "output_type": "stream"
        },
        {
          "execution_count": 28,
          "output_type": "execute_result",
          "data": {
            "text/plain": "('./merged_model_V7_merged_all_IEEEdatasets_threat/tokenizer_config.json',\n './merged_model_V7_merged_all_IEEEdatasets_threat/special_tokens_map.json',\n './merged_model_V7_merged_all_IEEEdatasets_threat/tokenizer.model',\n './merged_model_V7_merged_all_IEEEdatasets_threat/added_tokens.json',\n './merged_model_V7_merged_all_IEEEdatasets_threat/tokenizer.json')"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model.push_to_hub(\"merged_model_V7_merged_all_IEEEdatasets_threat\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T07:54:37.868113Z",
          "iopub.execute_input": "2024-04-20T07:54:37.868407Z",
          "iopub.status.idle": "2024-04-20T07:58:57.254695Z",
          "shell.execute_reply.started": "2024-04-20T07:54:37.868383Z",
          "shell.execute_reply": "2024-04-20T07:58:57.253715Z"
        },
        "trusted": true,
        "id": "RStjcELV9HBn",
        "outputId": "bfa059d5-602e-41d0-86b5-2d6cf7abac61",
        "colab": {
          "referenced_widgets": [
            "3b7ec45457624a9eae149783ccbadabe",
            "6597e87eae3d4c6183ddfd9b5ea07afe",
            "b39ed2fbe4e74335a692eca4fec43367",
            "e12032b3da564eeab40dbe7f8e7d7978"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b7ec45457624a9eae149783ccbadabe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6597e87eae3d4c6183ddfd9b5ea07afe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b39ed2fbe4e74335a692eca4fec43367"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e12032b3da564eeab40dbe7f8e7d7978"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 29,
          "output_type": "execute_result",
          "data": {
            "text/plain": "CommitInfo(commit_url='https://huggingface.co/AmirlyPhd/merged_model_V7_merged_all_IEEEdatasets_threat/commit/e1f8a72c639ee65ccd3d3bb3e7fbf515bf4cbd76', commit_message='Upload LlamaForCausalLM', commit_description='', oid='e1f8a72c639ee65ccd3d3bb3e7fbf515bf4cbd76', pr_url=None, pr_revision=None, pr_num=None)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "kTsjyi-M9HBo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "uQqEVYXX9HBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will first predict the sentiment labels for the test set using the predict() function. Then, it will evaluate the model's performance on the test set using the evaluate() function. The result now should be impressive with an overall accuracy of over 0.8 and high accuracy, precision and recall for the single sentiment labels. The prediction of the neutral label can still be improved, yet it is impressive how much could be done with little data and some fine-tuning."
      ],
      "metadata": {
        "id": "aJ1eYPoX9HBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = predict(X_test, merged_model, tokenizer)\n",
        "evaluation(y_true, y_pred)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T07:58:57.256982Z",
          "iopub.execute_input": "2024-04-20T07:58:57.257285Z",
          "iopub.status.idle": "2024-04-20T08:09:20.511092Z",
          "shell.execute_reply.started": "2024-04-20T07:58:57.257258Z",
          "shell.execute_reply": "2024-04-20T08:09:20.510141Z"
        },
        "trusted": true,
        "id": "XNlRe2Fe9HBp",
        "outputId": "9c64f71b-b1ee-4484-b0f5-a2045724b9fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|██████████| 1528/1528 [10:23<00:00,  2.45it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Accuracy: 0.651\nAccuracy for label 0: 0.040\nAccuracy for label 1: 0.880\nAccuracy for label 2: 0.760\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.30      0.04      0.07       329\n           1       0.65      0.88      0.75       583\n           2       0.67      0.76      0.71       616\n\n    accuracy                           0.65      1528\n   macro avg       0.54      0.56      0.51      1528\nweighted avg       0.58      0.65      0.59      1528\n\n\nConfusion Matrix:\n[[ 13 147 169]\n [  7 513  63]\n [ 23 125 468]]\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_test_prompt2(data_point):\n",
        "    return f\"\"\"\n",
        "            You are an intelligent chatbot. Analyze the following text and return the answer as the corresponding label \"irrelevant\" or one of the following cybercrime types: \"threat: 'vulnerability'\", \"threat: 'general'\", \"threat: 'ransomware'\", \"threat: 'ddos'\", \"threat: 'Offensive_Language'\", \"threat: 'botnet'\".\n",
        "\n",
        "            [{data_point[\"text\"]}]\n",
        "            \"\"\".strip()\n",
        "\n",
        "\n",
        "# Define some new text data points\n",
        "new_data = [\n",
        "    \"I just received an email asking for my bank account information. This seems suspicious ransomware.\",\n",
        "    \"Here's a link to a great resource for learning about cyber security!\",\n",
        "    \"I'm being threatened online! What should I do?\",\n",
        "]\n",
        "\n",
        "# Preprocess the new data (similar to how you processed X_test)\n",
        "new_data_df = pd.DataFrame({\"text\": new_data})\n",
        "new_data_prompts = new_data_df.apply(generate_test_prompt2, axis=1)  # Replace with your prompt generation function\n",
        "new_data_df = pd.DataFrame(new_data_prompts, columns=[\"text\"])\n",
        "\n",
        "# Make predictions on the new data\n",
        "new_predictions = predict(new_data_df, merged_model, tokenizer)\n",
        "\n",
        "# Print the predictions\n",
        "for i, text in enumerate(new_data):\n",
        "  print(f\"Text: {text}\")\n",
        "  print(f\"Predicted Label: {new_predictions[i]}\")\n",
        "  print(\"-------\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T02:21:53.848127Z",
          "iopub.execute_input": "2024-04-16T02:21:53.848813Z",
          "iopub.status.idle": "2024-04-16T02:21:55.31509Z",
          "shell.execute_reply.started": "2024-04-16T02:21:53.848781Z",
          "shell.execute_reply": "2024-04-16T02:21:55.314165Z"
        },
        "trusted": true,
        "id": "jI3PccpA9HBs",
        "outputId": "b7598a12-eceb-46ea-b6b6-d28a646e1dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|██████████| 3/3 [00:01<00:00,  2.07it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Text: I just received an email asking for my bank account information. This seems suspicious ransomware.\nPredicted Label: none\n-------\nText: Here's a link to a great resource for learning about cyber security!\nPredicted Label: irrelevant\n-------\nText: I'm being threatened online! What should I do?\nPredicted Label: irrelevant\n-------\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will create a Pandas DataFrame called evaluation containing the text, true labels, and predicted labels from the test set. This is expectially useful for understanding the errors that the fine-tuned model makes, and gettting insights on how to improve the prompt."
      ],
      "metadata": {
        "id": "tNtiKsC-9HBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation = pd.DataFrame({'text': X_test[\"text\"],\n",
        "                           'y_true':y_true,\n",
        "                           'y_pred': y_pred},\n",
        "                         )\n",
        "evaluation.to_csv(\"test_predictions.csv\", index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T08:09:06.278108Z",
          "iopub.status.idle": "2024-04-15T08:09:06.278432Z",
          "shell.execute_reply.started": "2024-04-15T08:09:06.278268Z",
          "shell.execute_reply": "2024-04-15T08:09:06.278281Z"
        },
        "trusted": true,
        "id": "ZZJbvQlu9HBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluation results are indeed good when compared to simpler benchmarks such as a CONV1D + bidirectional LSTM based model () such as: https://www.kaggle.com/code/lucamassaron/lstm-baseline-for-sentiment-analysis"
      ],
      "metadata": {
        "id": "Fd6W6sUJ9HBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the results of the baseline model:\n",
        "\n",
        "Accuracy: 0.623\n",
        "Accuracy for label 0: 0.620\n",
        "Accuracy for label 1: 0.590\n",
        "Accuracy for label 2: 0.660\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.79      0.62      0.69       300\n",
        "           1       0.61      0.59      0.60       300\n",
        "           2       0.53      0.66      0.59       300\n",
        "\n",
        "    accuracy                           0.62       900\n",
        "   macro avg       0.64      0.62      0.63       900\n",
        "weighted avg       0.64      0.62      0.63       900\n",
        "\n",
        "\n",
        "Confusion Matrix:\n",
        "\n",
        "[[186  39  75]\\\n",
        " [ 23 177 100]\\\n",
        " [ 27  75 198]]\n",
        ""
      ],
      "metadata": {
        "id": "w7n-yngt9HBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, LlamaTokenizerFast\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"/kaggle/working/merged_model_V4_merged_all_IEEEdatasets_threat\")\n",
        "tokenizer = LlamaTokenizerFast.from_pretrained(\"/kaggle/working/merged_model_V4_merged_all_IEEEdatasets_threat\")\n",
        "\n",
        "# ... rest of your code\n",
        "\n",
        "\n",
        "text_input = \"i will kill you.\"\n",
        "inputs = tokenizer(text_input, return_tensors=\"pt\")  # Tokenize and convert to tensors\n",
        "\n",
        "# Adjust parameters for generation\n",
        "generated_text = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    max_length=50,\n",
        "    temperature=0.7,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "generated_sequences = tokenizer.batch_decode(generated_text, skip_special_tokens=True)\n",
        "print(generated_sequences)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T08:14:29.245187Z",
          "iopub.execute_input": "2024-04-15T08:14:29.24563Z",
          "iopub.status.idle": "2024-04-15T08:16:19.811179Z",
          "shell.execute_reply.started": "2024-04-15T08:14:29.245602Z",
          "shell.execute_reply": "2024-04-15T08:16:19.809209Z"
        },
        "trusted": true,
        "id": "gB84mrNN9HBz",
        "outputId": "7a261c88-e517-49e9-e98f-6fac9b1d971e",
        "colab": {
          "referenced_widgets": [
            "2d3b7ed78c424fbf8cae6cb0acc7d973"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d3b7ed78c424fbf8cae6cb0acc7d973"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[\"i will kill you. Analyzing the 'I will kill you' speech\\nThe 'I will kill you' speech is one of the most powerful speeches in the movie. The speech is used to describe the power of a person'\"]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes as bnb\n",
        "\n",
        "from transformers import LlamaForConditionalGeneration, LlamaTokenizer  # Import the correct classes for Llama\n",
        "\n",
        "# Assuming you have an internet connection\n",
        "model_name = \"AmirlyPhd/v4_trained_weigths\"  # Replace with the actual Hub path if different\n",
        "\n",
        "model = LlamaForConditionalGeneration.from_pretrained(model_name)  # Load using LlamaForConditionalGeneration\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
        "\n",
        "example_tweet = \"I'm going to blow up this building! #angry\"  # Replace with your desired tweet\n",
        "\n",
        "# Assuming your generate_test_prompt function is compatible with Llama\n",
        "prompt = generate_test_prompt(data_point={\"text\": example_tweet})\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "output = model.generate(input_ids, max_length=50, num_beams=5)  # Adjust max_length as needed\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)\n",
        "\n",
        "\n",
        "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# # Assuming you have an internet connection\n",
        "# model_name = \"AmirlyPhd/trained_weigths\"  # Replace with the actual Hub path if different\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# example_tweet = \"I'm going to blow up this building! #angry\"  # Replace with your desired tweet\n",
        "\n",
        "# prompt = generate_test_prompt(data_point={\"text\": example_tweet})  # Use your `generate_test_prompt` function\n",
        "\n",
        "# input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# output = model.generate(input_ids, max_length=50, num_beams=5)  # Adjust max_length as needed\n",
        "# generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "# print(generated_text)\n"
      ],
      "metadata": {
        "id": "S7Vdllqm9HB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline  # Assuming pipeline is from transformers\n",
        "\n",
        "# def predict_sentiment(tweet, merged_model, tokenizer):\n",
        "#   \"\"\"Predicts the sentiment of a single tweet using the provided model and tokenizer.\"\"\"\n",
        "\n",
        "#   prompt = tweet  # Directly use the given tweet as the prompt\n",
        "\n",
        "#   pipe = pipeline(task=\"text-generation\",  # Use a pipeline for convenience\n",
        "#                   model=merged_model,  # Replace with your compatible text-generation model\n",
        "#                   tokenizer=tokenizer,\n",
        "#                   max_new_tokens=1,\n",
        "#                   temperature=0.0)\n",
        "\n",
        "#   result = pipe(prompt)\n",
        "#   answer = result[0]['generated_text'].split(\"=\")[-1]\n",
        "\n",
        "#   if \"threat\" in answer:\n",
        "#       return \"normal\"\n",
        "#   else:\n",
        "#       return \"threat\"\n",
        "\n",
        "# # Example usage:\n",
        "# tweet = \"i will kill you\"\n",
        "# predicted_sentiment = predict_sentiment(tweet, merged_model, tokenizer)\n",
        "# print(f\"Predicted sentiment for tweet: {predicted_sentiment}\")\n",
        "\n",
        "# # Assuming the model and tokenizer are loaded and saved (replace with your actual loading logic)\n",
        "\n",
        "# def predict_sentiment(tweet_text):\n",
        "#   \"\"\"\n",
        "#   This function takes a tweet text as input and predicts its sentiment using the loaded Peft model.\n",
        "#   \"\"\"\n",
        "#   device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # Check for GPU availability\n",
        "\n",
        "#   # Preprocess the tweet text using the tokenizer, ensuring it's on the same device as the model (implicitly)\n",
        "#   inputs = tokenizer(tweet_text, return_tensors=\"pt\", truncation=True, max_length=512)  # Truncate to avoid exceeding model limits\n",
        "\n",
        "#   # Get the model's prediction on the tweet\n",
        "#   with torch.no_grad():\n",
        "#     outputs = merged_model(**inputs)\n",
        "#     logits = outputs[0]  # Assuming logits are the last element of the model's output\n",
        "\n",
        "#   # Get the predicted sentiment class based on the highest probability\n",
        "#   predicted_class = torch.argmax(logits, dim=-1).item()  # Get the index of the maximum value\n",
        "\n",
        "#   # Map the predicted class index to a sentiment label (replace with your actual mapping)\n",
        "#   sentiment_map = {0: \"threat\", 1: \"normal\", 2: \"unknown\"}\n",
        "#   predicted_sentiment = sentiment_map[predicted_class]\n",
        "\n",
        "#   return predicted_sentiment\n",
        "\n",
        "\n",
        "\n",
        "# # Sample tweet to test\n",
        "# tweet = \"I'm so frustrated with this long wait time at the customer service center. #angry #waiting\"\n",
        "\n",
        "# # Predict sentiment\n",
        "# predicted_sentiment = predict_sentiment(tweet)\n",
        "\n",
        "# print(f\"Predicted sentiment for tweet: {predicted_sentiment}\")\n"
      ],
      "metadata": {
        "id": "LnzPwtPs9HB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this testing, the fine-tuning of Llama 2 has reached its conclusion. Dont't forget to upvote if you find the notebook useful for your projects or work!"
      ],
      "metadata": {
        "id": "mTfO2wQY9HB1"
      }
    }
  ]
}